{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a06ad505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84680165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(excluded_session):\n",
    "    train_text = torch.load(f\"./transcriptions/output_{excluded_session}/train_text_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    train_speech = torch.load(f\"./speech/output_{excluded_session}/train_speech_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    train_video = torch.load(f\"./video/output_{excluded_session}/train_video_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    summed_data = train_text + train_speech + train_video\n",
    "    \n",
    "    train_labels = torch.load(f\"./transcriptions/output_{excluded_session}/train_text_labels{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    \n",
    "    test_text = torch.load(f\"./transcriptions/output_{excluded_session}/test_text_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    test_speech = torch.load(f\"./speech/output_{excluded_session}/test_speech_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    test_video = torch.load(f\"./video/output_{excluded_session}/test_video_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    summed_test_data = test_text + test_speech + test_video\n",
    "\n",
    "    test_labels = torch.load(f\"./transcriptions/output_{excluded_session}/test_text_labels{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    \n",
    "    return summed_data, train_labels, summed_test_data, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c0d3ce",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1f7c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def SVM_results(excluded_session):\n",
    "    \n",
    "    print(f\"Session{excluded_session} is used as test set.\")\n",
    "    x_train, y_train, x_test, y_test = load_data(excluded_session)\n",
    "    \n",
    "    clf = svm.SVC()\n",
    "    clf.fit(x_train, y_train)\n",
    "    \n",
    "    target_names = ['ang', 'hap', 'neu', 'sad']\n",
    "\n",
    "    train_predicted = clf.predict(x_train)\n",
    "    print('*** Train')\n",
    "    print(classification_report(y_train, train_predicted, target_names=target_names, digits=6))\n",
    "    print('*** Test')\n",
    "    test_predicted = clf.predict(x_test)\n",
    "    print(classification_report(y_test, test_predicted, target_names=target_names, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15ef1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session1 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.984091  0.990847  0.987457       874\n",
      "         hap   0.972953  0.980118  0.976522      1358\n",
      "         neu   0.967424  0.964502  0.965961      1324\n",
      "         sad   0.979499  0.966292  0.972851       890\n",
      "\n",
      "    accuracy                       0.974809      4446\n",
      "   macro avg   0.975992  0.975440  0.975698      4446\n",
      "weighted avg   0.974806  0.974809  0.974792      4446\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.896714  0.834061  0.864253       229\n",
      "         hap   0.706587  0.848921  0.771242       278\n",
      "         neu   0.787456  0.588542  0.673621       384\n",
      "         sad   0.629482  0.814433  0.710112       194\n",
      "\n",
      "    accuracy                       0.747465      1085\n",
      "   macro avg   0.755060  0.771489  0.754807      1085\n",
      "weighted avg   0.761550  0.747465  0.745393      1085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b74ff8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session2 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.972393  0.984472  0.978395       966\n",
      "         hap   0.973077  0.966387  0.969720      1309\n",
      "         neu   0.958395  0.958395  0.958395      1346\n",
      "         sad   0.964932  0.961669  0.963298       887\n",
      "\n",
      "    accuracy                       0.966948      4508\n",
      "   macro avg   0.967199  0.967731  0.967452      4508\n",
      "weighted avg   0.966944  0.966948  0.966934      4508\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.848684  0.941606  0.892734       137\n",
      "         hap   0.792169  0.804281  0.798179       327\n",
      "         neu   0.771930  0.729282  0.750000       362\n",
      "         sad   0.812183  0.812183  0.812183       197\n",
      "\n",
      "    accuracy                       0.797654      1023\n",
      "   macro avg   0.806241  0.821838  0.813274      1023\n",
      "weighted avg   0.796430  0.797654  0.796490      1023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c5b2f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session3 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.979118  0.977984  0.978551       863\n",
      "         hap   0.972305  0.962222  0.967238      1350\n",
      "         neu   0.949249  0.956772  0.952996      1388\n",
      "         sad   0.962963  0.967908  0.965429       779\n",
      "\n",
      "    accuracy                       0.964612      4380\n",
      "   macro avg   0.965909  0.966221  0.966053      4380\n",
      "weighted avg   0.964680  0.964612  0.964632      4380\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.750000  0.787500  0.768293       240\n",
      "         hap   0.814229  0.720280  0.764378       286\n",
      "         neu   0.633523  0.696875  0.663690       320\n",
      "         sad   0.772109  0.744262  0.757930       305\n",
      "\n",
      "    accuracy                       0.734144      1151\n",
      "   macro avg   0.742465  0.737229  0.738573      1151\n",
      "weighted avg   0.739435  0.734144  0.735493      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f701065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session4 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.980818  0.988402  0.984596       776\n",
      "         hap   0.975940  0.973743  0.974840      1333\n",
      "         neu   0.962069  0.962069  0.962069      1450\n",
      "         sad   0.969083  0.965994  0.967536       941\n",
      "\n",
      "    accuracy                       0.970889      4500\n",
      "   macro avg   0.971978  0.972552  0.972260      4500\n",
      "weighted avg   0.970878  0.970889  0.970880      4500\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.778364  0.902141  0.835694       327\n",
      "         hap   0.770909  0.699670  0.733564       303\n",
      "         neu   0.658915  0.658915  0.658915       258\n",
      "         sad   0.882353  0.734266  0.801527       143\n",
      "\n",
      "    accuracy                       0.758487      1031\n",
      "   macro avg   0.772635  0.748748  0.757425      1031\n",
      "weighted avg   0.760705  0.758487  0.756702      1031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f133480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session5 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.982833  0.981779  0.982306       933\n",
      "         hap   0.969396  0.981575  0.975447      1194\n",
      "         neu   0.955572  0.958459  0.957014      1324\n",
      "         sad   0.976857  0.955900  0.966265       839\n",
      "\n",
      "    accuracy                       0.969464      4290\n",
      "   macro avg   0.971165  0.969428  0.970258      4290\n",
      "weighted avg   0.969511  0.969464  0.969454      4290\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.702020  0.817647  0.755435       170\n",
      "         hap   0.803571  0.712670  0.755396       442\n",
      "         neu   0.652452  0.796875  0.717468       384\n",
      "         sad   0.824176  0.612245  0.702576       245\n",
      "\n",
      "    accuracy                       0.733280      1241\n",
      "   macro avg   0.745555  0.734859  0.732719      1241\n",
      "weighted avg   0.746967  0.733280  0.733237      1241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f25838c",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "745f259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_predict(model, d_loader):\n",
    "    predicted_set = []\n",
    "\n",
    "    # no need to calculate gradients during inference\n",
    "    with torch.no_grad():\n",
    "      for data in d_loader:\n",
    "        inputs, labels = data\n",
    "        # calculate output by running through the network\n",
    "        outputs = model(inputs)\n",
    "        # get the predictions\n",
    "        __, predicted = torch.max(outputs.data, 1)\n",
    "        predicted_set += predicted.tolist()\n",
    "    \n",
    "    return predicted_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de0c65bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_train(model, t_loader):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "    \n",
    "    epochs = 50\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(t_loader, 0):\n",
    "            inputs, labels = data\n",
    "            # set optimizer to zero grad to remove previous epoch gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward propagation\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # backward propagation\n",
    "            loss.backward()\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57916356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Data(Dataset):\n",
    "  def __init__(self, X_train, y_train):\n",
    "    # need to convert float64 to float32 else \n",
    "    # will get the following error\n",
    "    # RuntimeError: expected scalar type Double but found Float\n",
    "    #self.X = torch.from_numpy(X_train.cast(np.float64))\n",
    "    self.X = X_train\n",
    "    # need to convert float64 to Long else \n",
    "    # will get the following error\n",
    "    # RuntimeError: expected scalar type Long but found Float\n",
    "    self.y = torch.FloatTensor(y_train).type(torch.LongTensor)\n",
    "    self.len = self.X.shape[0]\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    return self.X[index], self.y[index]\n",
    "  def __len__(self):\n",
    "    return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cec3f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# number of features (len of X cols)\n",
    "# number of features (len of X cols)\n",
    "input_dim = 768\n",
    "# number of hidden layers\n",
    "hidden_layers = 256\n",
    "# number of classes (unique of y)\n",
    "output_dim = 4\n",
    "class Network(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Network, self).__init__()\n",
    "    self.linear1 = nn.Linear(input_dim, hidden_layers)\n",
    "    self.linear2 = nn.Linear(hidden_layers, output_dim)\n",
    "  def forward(self, x):\n",
    "    x = torch.sigmoid(self.linear1(x))\n",
    "    x = self.linear2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76fce30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def NN_results(excluded_session):\n",
    "    \n",
    "    print(f\"Session{excluded_session} is used as test set.\")\n",
    "    x_train, y_train, x_test, y_test = load_data(excluded_session)\n",
    "    \n",
    "    train_data = Data(x_train, y_train)\n",
    "    batch_size = 32\n",
    "    trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    clf = Network()\n",
    "    print(clf.parameters)\n",
    "    \n",
    "    NN_train(clf, trainloader)\n",
    "    \n",
    "    test_data = Data(x_test, y_test)\n",
    "    testloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    target_names = ['ang', 'hap', 'neu', 'sad']\n",
    "\n",
    "    train_predicted = NN_predict(clf, trainloader)\n",
    "    print('*** Train')\n",
    "    print(classification_report(y_train, train_predicted, target_names=target_names, digits=6))\n",
    "    print('*** Test')\n",
    "    test_predicted = NN_predict(clf, testloader)\n",
    "    print(classification_report(y_test, test_predicted, target_names=target_names, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b70a6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session1 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (linear2): Linear(in_features=256, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.990847  0.990847  0.990847       874\n",
      "         hap   0.987472  0.986745  0.987109      1358\n",
      "         neu   0.975958  0.981118  0.978531      1324\n",
      "         sad   0.986425  0.979775  0.983089       890\n",
      "\n",
      "    accuracy                       0.984480      4446\n",
      "   macro avg   0.985176  0.984621  0.984894      4446\n",
      "weighted avg   0.984497  0.984480  0.984484      4446\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.917476  0.825328  0.868966       229\n",
      "         hap   0.722930  0.816547  0.766892       278\n",
      "         neu   0.752351  0.625000  0.682788       384\n",
      "         sad   0.634146  0.804124  0.709091       194\n",
      "\n",
      "    accuracy                       0.748387      1085\n",
      "   macro avg   0.756726  0.767749  0.756934      1085\n",
      "weighted avg   0.758529  0.748387  0.748335      1085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86a19953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session2 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (linear2): Linear(in_features=256, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.998947  0.982402  0.990605       966\n",
      "         hap   0.974398  0.988541  0.981418      1309\n",
      "         neu   0.979342  0.950966  0.964945      1346\n",
      "         sad   0.951246  0.989853  0.970166       887\n",
      "\n",
      "    accuracy                       0.976264      4508\n",
      "   macro avg   0.975983  0.977940  0.976784      4508\n",
      "weighted avg   0.976579  0.976264  0.976254      4508\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.934959  0.839416  0.884615       137\n",
      "         hap   0.756164  0.844037  0.797688       327\n",
      "         neu   0.775385  0.696133  0.733624       362\n",
      "         sad   0.800000  0.852792  0.825553       197\n",
      "\n",
      "    accuracy                       0.792766      1023\n",
      "   macro avg   0.816627  0.808094  0.810370      1023\n",
      "weighted avg   0.795351  0.792766  0.792026      1023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f08ea83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session3 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (linear2): Linear(in_features=256, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.997674  0.994206  0.995937       863\n",
      "         hap   0.989482  0.975556  0.982469      1350\n",
      "         neu   0.961837  0.980548  0.971102      1388\n",
      "         sad   0.978036  0.971759  0.974887       779\n",
      "\n",
      "    accuracy                       0.980137      4380\n",
      "   macro avg   0.981757  0.980517  0.981099      4380\n",
      "weighted avg   0.980300  0.980137  0.980172      4380\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.788793  0.762500  0.775424       240\n",
      "         hap   0.814655  0.660839  0.729730       286\n",
      "         neu   0.612987  0.737500  0.669504       320\n",
      "         sad   0.778146  0.770492  0.774300       305\n",
      "\n",
      "    accuracy                       0.732407      1151\n",
      "   macro avg   0.748645  0.732833  0.737239      1151\n",
      "weighted avg   0.743520  0.732407  0.734324      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0a83eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session4 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (linear2): Linear(in_features=256, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.993532  0.989691  0.991607       776\n",
      "         hap   0.987131  0.978245  0.982668      1333\n",
      "         neu   0.973793  0.973793  0.973793      1450\n",
      "         sad   0.972803  0.988310  0.980496       941\n",
      "\n",
      "    accuracy                       0.980889      4500\n",
      "   macro avg   0.981815  0.982510  0.982141      4500\n",
      "weighted avg   0.980941  0.980889  0.980895      4500\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.778378  0.880734  0.826399       327\n",
      "         hap   0.778626  0.673267  0.722124       303\n",
      "         neu   0.646840  0.674419  0.660342       258\n",
      "         sad   0.846154  0.769231  0.805861       143\n",
      "\n",
      "    accuracy                       0.752667      1031\n",
      "   macro avg   0.762500  0.749413  0.753681      1031\n",
      "weighted avg   0.754935  0.752667  0.751350      1031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce84e315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session5 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (linear2): Linear(in_features=256, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.987220  0.993569  0.990385       933\n",
      "         hap   0.989813  0.976549  0.983137      1194\n",
      "         neu   0.965697  0.978097  0.971857      1324\n",
      "         sad   0.983173  0.974970  0.979054       839\n",
      "\n",
      "    accuracy                       0.980420      4290\n",
      "   macro avg   0.981476  0.980796  0.981108      4290\n",
      "weighted avg   0.980508  0.980420  0.980434      4290\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.672986  0.835294  0.745407       170\n",
      "         hap   0.833795  0.680995  0.749689       442\n",
      "         neu   0.640496  0.807292  0.714286       384\n",
      "         sad   0.837838  0.632653  0.720930       245\n",
      "\n",
      "    accuracy                       0.731668      1241\n",
      "   macro avg   0.746279  0.739059  0.732578      1241\n",
      "weighted avg   0.752752  0.731668  0.732470      1241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffd44f0",
   "metadata": {},
   "source": [
    "# xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4ffc066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def XGBresults(excluded_session):\n",
    "    \n",
    "    print(f\"Session{excluded_session} is used as test set.\")\n",
    "    x_train, y_train, x_test, y_test = load_data(excluded_session)\n",
    "    \n",
    "    bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic')\n",
    "    bst.fit(x_train, y_train)\n",
    "    \n",
    "    target_names = ['ang', 'hap', 'neu', 'sad']\n",
    "\n",
    "    train_predicted = bst.predict(x_train)\n",
    "    print('*** Train')\n",
    "    print(classification_report(y_train, train_predicted, target_names=target_names, digits=6))\n",
    "    print('*** Test')\n",
    "    test_predicted = bst.predict(x_test)\n",
    "    print(classification_report(y_test, test_predicted, target_names=target_names, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce83d578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session1 is used as test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:04:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.985006  0.977117  0.981045       874\n",
      "         hap   0.959184  0.969072  0.964103      1358\n",
      "         neu   0.946975  0.957704  0.952309      1324\n",
      "         sad   0.966590  0.942697  0.954494       890\n",
      "\n",
      "    accuracy                       0.961988      4446\n",
      "   macro avg   0.964439  0.961647  0.962988      4446\n",
      "weighted avg   0.962107  0.961988  0.961998      4446\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.915344  0.755459  0.827751       229\n",
      "         hap   0.682099  0.794964  0.734219       278\n",
      "         neu   0.714715  0.619792  0.663877       384\n",
      "         sad   0.619247  0.762887  0.683603       194\n",
      "\n",
      "    accuracy                       0.718894      1085\n",
      "   macro avg   0.732851  0.733275  0.727363      1085\n",
      "weighted avg   0.731633  0.718894  0.720015      1085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fee02897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session2 is used as test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:04:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.975207  0.977226  0.976215       966\n",
      "         hap   0.971451  0.961803  0.966603      1309\n",
      "         neu   0.948454  0.956909  0.952663      1346\n",
      "         sad   0.960497  0.959414  0.959955       887\n",
      "\n",
      "    accuracy                       0.963177      4508\n",
      "   macro avg   0.963902  0.963838  0.963859      4508\n",
      "weighted avg   0.963234  0.963177  0.963192      4508\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.852113  0.883212  0.867384       137\n",
      "         hap   0.786378  0.776758  0.781538       327\n",
      "         neu   0.749296  0.734807  0.741980       362\n",
      "         sad   0.798030  0.822335  0.810000       197\n",
      "\n",
      "    accuracy                       0.784946      1023\n",
      "   macro avg   0.796454  0.804278  0.800226      1023\n",
      "weighted avg   0.784303  0.784946  0.784518      1023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "557672c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session3 is used as test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:04:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.967555  0.967555  0.967555       863\n",
      "         hap   0.959429  0.945926  0.952630      1350\n",
      "         neu   0.931034  0.933718  0.932374      1388\n",
      "         sad   0.937028  0.955071  0.945963       779\n",
      "\n",
      "    accuracy                       0.947945      4380\n",
      "   macro avg   0.948762  0.950567  0.949630      4380\n",
      "weighted avg   0.948048  0.947945  0.947966      4380\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.790179  0.737500  0.762931       240\n",
      "         hap   0.773810  0.681818  0.724907       286\n",
      "         neu   0.608466  0.718750  0.659026       320\n",
      "         sad   0.757576  0.737705  0.747508       305\n",
      "\n",
      "    accuracy                       0.718506      1151\n",
      "   macro avg   0.732507  0.718943  0.723593      1151\n",
      "weighted avg   0.726952  0.718506  0.720508      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2163fd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session4 is used as test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:04:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.988142  0.966495  0.977199       776\n",
      "         hap   0.966114  0.962491  0.964299      1333\n",
      "         neu   0.947512  0.958621  0.953034      1450\n",
      "         sad   0.957717  0.962806  0.960254       941\n",
      "\n",
      "    accuracy                       0.962000      4500\n",
      "   macro avg   0.964871  0.962603  0.963697      4500\n",
      "weighted avg   0.962163  0.962000  0.962048      4500\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.767045  0.825688  0.795287       327\n",
      "         hap   0.785992  0.666667  0.721429       303\n",
      "         neu   0.615120  0.693798  0.652095       258\n",
      "         sad   0.839695  0.769231  0.802920       143\n",
      "\n",
      "    accuracy                       0.738118      1031\n",
      "   macro avg   0.751963  0.738846  0.742933      1031\n",
      "weighted avg   0.744672  0.738118  0.738807      1031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc4aebb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session5 is used as test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:14:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.985946  0.977492  0.981701       933\n",
      "         hap   0.973840  0.966499  0.970156      1194\n",
      "         neu   0.937962  0.959215  0.948469      1324\n",
      "         sad   0.963680  0.948749  0.956156       839\n",
      "\n",
      "    accuracy                       0.963170      4290\n",
      "   macro avg   0.965357  0.962989  0.964120      4290\n",
      "weighted avg   0.963413  0.963170  0.963236      4290\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.756906  0.805882  0.780627       170\n",
      "         hap   0.834688  0.696833  0.759556       442\n",
      "         neu   0.631263  0.820312  0.713477       384\n",
      "         sad   0.807292  0.632653  0.709382       245\n",
      "\n",
      "    accuracy                       0.737309      1241\n",
      "   macro avg   0.757537  0.738920  0.740760      1241\n",
      "weighted avg   0.755679  0.737309  0.738279      1241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4269f678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ba5e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
