{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a06ad505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84680165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(excluded_session):\n",
    "    train_text = torch.load(f\"./transcriptions/output_{excluded_session}/train_text_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    train_speech = torch.load(f\"./speech/output_{excluded_session}/train_speech_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    train_video = torch.load(f\"./video/output_{excluded_session}/train_video_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    summed_data = train_text * train_speech * train_video\n",
    "    \n",
    "    train_labels = torch.load(f\"./transcriptions/output_{excluded_session}/train_text_labels{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    \n",
    "    test_text = torch.load(f\"./transcriptions/output_{excluded_session}/test_text_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    test_speech = torch.load(f\"./speech/output_{excluded_session}/test_speech_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    test_video = torch.load(f\"./video/output_{excluded_session}/test_video_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    summed_test_data = test_text * test_speech * test_video\n",
    "\n",
    "    test_labels = torch.load(f\"./transcriptions/output_{excluded_session}/test_text_labels{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    \n",
    "    return summed_data, train_labels, summed_test_data, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c0d3ce",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1f7c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def SVM_results(excluded_session):\n",
    "    \n",
    "    print(f\"Session{excluded_session} is used as test set.\")\n",
    "    x_train, y_train, x_test, y_test = load_data(excluded_session)\n",
    "    \n",
    "    clf = svm.SVC()\n",
    "    clf.fit(x_train, y_train)\n",
    "    \n",
    "    target_names = ['ang', 'hap', 'neu', 'sad']\n",
    "\n",
    "    train_predicted = clf.predict(x_train)\n",
    "    print('*** Train')\n",
    "    print(classification_report(y_train, train_predicted, target_names=target_names, digits=6))\n",
    "    print('*** Test')\n",
    "    test_predicted = clf.predict(x_test)\n",
    "    print(classification_report(y_test, test_predicted, target_names=target_names, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f15ef1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session1 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.982896  0.986270  0.984580       874\n",
      "         hap   0.973723  0.982327  0.978006      1358\n",
      "         neu   0.964635  0.968278  0.966453      1324\n",
      "         sad   0.981609  0.959551  0.970455       890\n",
      "\n",
      "    accuracy                       0.974359      4446\n",
      "   macro avg   0.975716  0.974106  0.974873      4446\n",
      "weighted avg   0.974398  0.974359  0.974346      4446\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.910448  0.799127  0.851163       229\n",
      "         hap   0.702454  0.823741  0.758278       278\n",
      "         neu   0.719626  0.601562  0.655319       384\n",
      "         sad   0.632911  0.773196  0.696056       194\n",
      "\n",
      "    accuracy                       0.730876      1085\n",
      "   macro avg   0.741360  0.749407  0.740204      1085\n",
      "weighted avg   0.739996  0.730876  0.730318      1085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b74ff8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session2 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.985122  0.959627  0.972208       966\n",
      "         hap   0.964340  0.970970  0.967644      1309\n",
      "         neu   0.937771  0.962853  0.950147      1346\n",
      "         sad   0.971165  0.949267  0.960091       887\n",
      "\n",
      "    accuracy                       0.961846      4508\n",
      "   macro avg   0.964600  0.960679  0.962522      4508\n",
      "weighted avg   0.962203  0.961846  0.961911      4508\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.875000  0.868613  0.871795       137\n",
      "         hap   0.750693  0.828746  0.787791       327\n",
      "         neu   0.742297  0.732044  0.737135       362\n",
      "         sad   0.840237  0.720812  0.775956       197\n",
      "\n",
      "    accuracy                       0.779081      1023\n",
      "   macro avg   0.802057  0.787554  0.793169      1023\n",
      "weighted avg   0.781612  0.779081  0.778836      1023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c5b2f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session3 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.966002  0.954809  0.960373       863\n",
      "         hap   0.944486  0.945185  0.944835      1350\n",
      "         neu   0.923404  0.938040  0.930665      1388\n",
      "         sad   0.953003  0.937099  0.944984       779\n",
      "\n",
      "    accuracy                       0.943379      4380\n",
      "   macro avg   0.946724  0.943783  0.945214      4380\n",
      "weighted avg   0.943559  0.943379  0.943433      4380\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.732218  0.729167  0.730689       240\n",
      "         hap   0.734082  0.685315  0.708861       286\n",
      "         neu   0.577957  0.671875  0.621387       320\n",
      "         sad   0.758242  0.678689  0.716263       305\n",
      "\n",
      "    accuracy                       0.688966      1151\n",
      "   macro avg   0.700625  0.691261  0.694300      1151\n",
      "weighted avg   0.696690  0.688966  0.691054      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f701065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session4 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.981865  0.976804  0.979328       776\n",
      "         hap   0.965723  0.972243  0.968972      1333\n",
      "         neu   0.952447  0.953103  0.952775      1450\n",
      "         sad   0.964706  0.958555  0.961620       941\n",
      "\n",
      "    accuracy                       0.964000      4500\n",
      "   macro avg   0.966185  0.965176  0.965674      4500\n",
      "weighted avg   0.964016  0.964000  0.964002      4500\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.767568  0.868502  0.814921       327\n",
      "         hap   0.761905  0.633663  0.691892       303\n",
      "         neu   0.608247  0.686047  0.644809       258\n",
      "         sad   0.830508  0.685315  0.750958       143\n",
      "\n",
      "    accuracy                       0.728419      1031\n",
      "   macro avg   0.742057  0.718382  0.725645      1031\n",
      "weighted avg   0.734765  0.728419  0.727323      1031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f133480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session5 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.988134  0.981779  0.984946       933\n",
      "         hap   0.964638  0.982412  0.973444      1194\n",
      "         neu   0.949254  0.960725  0.954955      1324\n",
      "         sad   0.980173  0.942789  0.961118       839\n",
      "\n",
      "    accuracy                       0.967832      4290\n",
      "   macro avg   0.970550  0.966926  0.968616      4290\n",
      "weighted avg   0.968038  0.967832  0.967829      4290\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.752747  0.805882  0.778409       170\n",
      "         hap   0.797436  0.703620  0.747596       442\n",
      "         neu   0.630832  0.809896  0.709236       384\n",
      "         sad   0.829545  0.595918  0.693587       245\n",
      "\n",
      "    accuracy                       0.729251      1241\n",
      "   macro avg   0.752640  0.728829  0.732207      1241\n",
      "weighted avg   0.746101  0.729251  0.729285      1241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f25838c",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "745f259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_predict(model, d_loader):\n",
    "    predicted_set = []\n",
    "\n",
    "    # no need to calculate gradients during inference\n",
    "    with torch.no_grad():\n",
    "      for data in d_loader:\n",
    "        inputs, labels = data\n",
    "        # calculate output by running through the network\n",
    "        outputs = model(inputs)\n",
    "        # get the predictions\n",
    "        __, predicted = torch.max(outputs.data, 1)\n",
    "        predicted_set += predicted.tolist()\n",
    "    \n",
    "    return predicted_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de0c65bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_train(model, t_loader):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "    \n",
    "    epochs = 50\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(t_loader, 0):\n",
    "            inputs, labels = data\n",
    "            # set optimizer to zero grad to remove previous epoch gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward propagation\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # backward propagation\n",
    "            loss.backward()\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57916356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Data(Dataset):\n",
    "  def __init__(self, X_train, y_train):\n",
    "    # need to convert float64 to float32 else \n",
    "    # will get the following error\n",
    "    # RuntimeError: expected scalar type Double but found Float\n",
    "    #self.X = torch.from_numpy(X_train.cast(np.float64))\n",
    "    self.X = X_train\n",
    "    # need to convert float64 to Long else \n",
    "    # will get the following error\n",
    "    # RuntimeError: expected scalar type Long but found Float\n",
    "    self.y = torch.FloatTensor(y_train).type(torch.LongTensor)\n",
    "    self.len = self.X.shape[0]\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    return self.X[index], self.y[index]\n",
    "  def __len__(self):\n",
    "    return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cec3f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# number of features (len of X cols)\n",
    "# number of features (len of X cols)\n",
    "input_dim = 768\n",
    "# number of hidden layers\n",
    "hidden_layers = 256\n",
    "# number of classes (unique of y)\n",
    "output_dim = 4\n",
    "class Network(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Network, self).__init__()\n",
    "    self.linear1 = nn.Linear(input_dim, hidden_layers)\n",
    "    self.linear2 = nn.Linear(hidden_layers, output_dim)\n",
    "  def forward(self, x):\n",
    "    x = torch.sigmoid(self.linear1(x))\n",
    "    x = self.linear2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76fce30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def NN_results(excluded_session):\n",
    "    \n",
    "    print(f\"Session{excluded_session} is used as test set.\")\n",
    "    x_train, y_train, x_test, y_test = load_data(excluded_session)\n",
    "    \n",
    "    train_data = Data(x_train, y_train)\n",
    "    batch_size = 32\n",
    "    trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    clf = Network()\n",
    "    print(clf.parameters)\n",
    "    \n",
    "    NN_train(clf, trainloader)\n",
    "    \n",
    "    test_data = Data(x_test, y_test)\n",
    "    testloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    target_names = ['ang', 'hap', 'neu', 'sad']\n",
    "\n",
    "    train_predicted = NN_predict(clf, trainloader)\n",
    "    print('*** Train')\n",
    "    print(classification_report(y_train, train_predicted, target_names=target_names, digits=6))\n",
    "    print('*** Test')\n",
    "    test_predicted = NN_predict(clf, testloader)\n",
    "    print(classification_report(y_test, test_predicted, target_names=target_names, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b70a6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session1 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (linear2): Linear(in_features=256, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.966857  0.967963  0.967410       874\n",
      "         hap   0.946237  0.972018  0.958954      1358\n",
      "         neu   0.957562  0.937311  0.947328      1324\n",
      "         sad   0.955682  0.944944  0.950282       890\n",
      "\n",
      "    accuracy                       0.955466      4446\n",
      "   macro avg   0.956584  0.955559  0.955994      4446\n",
      "weighted avg   0.955554  0.955466  0.955418      4446\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.869792  0.729258  0.793349       229\n",
      "         hap   0.652299  0.816547  0.725240       278\n",
      "         neu   0.714765  0.554688  0.624633       384\n",
      "         sad   0.623482  0.793814  0.698413       194\n",
      "\n",
      "    accuracy                       0.701382      1085\n",
      "   macro avg   0.715084  0.723577  0.710409      1085\n",
      "weighted avg   0.715158  0.701382  0.699212      1085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86a19953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session2 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (linear2): Linear(in_features=256, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.969247  0.946170  0.957569       966\n",
      "         hap   0.930882  0.967150  0.948670      1309\n",
      "         neu   0.939462  0.933878  0.936662      1346\n",
      "         sad   0.958478  0.936866  0.947548       887\n",
      "\n",
      "    accuracy                       0.946761      4508\n",
      "   macro avg   0.949517  0.946016  0.947612      4508\n",
      "weighted avg   0.947095  0.946761  0.946771      4508\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.837037  0.824818  0.830882       137\n",
      "         hap   0.704787  0.810398  0.753912       327\n",
      "         neu   0.755952  0.701657  0.727794       362\n",
      "         sad   0.829545  0.741117  0.782842       197\n",
      "\n",
      "    accuracy                       0.760508      1023\n",
      "   macro avg   0.781831  0.769497  0.773857      1023\n",
      "weighted avg   0.764628  0.760508  0.760549      1023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f08ea83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session3 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (linear2): Linear(in_features=256, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.941176  0.964079  0.952490       863\n",
      "         hap   0.946975  0.939259  0.943102      1350\n",
      "         neu   0.918324  0.931556  0.924893      1388\n",
      "         sad   0.954606  0.917843  0.935864       779\n",
      "\n",
      "    accuracy                       0.937900      4380\n",
      "   macro avg   0.940270  0.938184  0.939087      4380\n",
      "weighted avg   0.938110  0.937900  0.937894      4380\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.661765  0.750000  0.703125       240\n",
      "         hap   0.751055  0.622378  0.680688       286\n",
      "         neu   0.561404  0.700000  0.623088       320\n",
      "         sad   0.802469  0.639344  0.711679       305\n",
      "\n",
      "    accuracy                       0.675065      1151\n",
      "   macro avg   0.694173  0.677930  0.679645      1151\n",
      "weighted avg   0.693334  0.675065  0.677565      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0a83eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session4 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (linear2): Linear(in_features=256, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.972621  0.961340  0.966948       776\n",
      "         hap   0.946667  0.958740  0.952665      1333\n",
      "         neu   0.933242  0.935172  0.934206      1450\n",
      "         sad   0.953763  0.942614  0.948156       941\n",
      "\n",
      "    accuracy                       0.948222      4500\n",
      "   macro avg   0.951573  0.949467  0.950494      4500\n",
      "weighted avg   0.948300  0.948222  0.948237      4500\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.834891  0.819572  0.827160       327\n",
      "         hap   0.700422  0.547855  0.614815       303\n",
      "         neu   0.519062  0.686047  0.590985       258\n",
      "         sad   0.765152  0.706294  0.734545       143\n",
      "\n",
      "    accuracy                       0.690592      1031\n",
      "   macro avg   0.704882  0.689942  0.691876      1031\n",
      "weighted avg   0.706665  0.690592  0.692807      1031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce84e315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session5 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (linear2): Linear(in_features=256, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.921529  0.981779  0.950701       933\n",
      "         hap   0.963652  0.954774  0.959192      1194\n",
      "         neu   0.935780  0.924471  0.930091      1324\n",
      "         sad   0.959006  0.920143  0.939173       839\n",
      "\n",
      "    accuracy                       0.944522      4290\n",
      "   macro avg   0.944992  0.945292  0.944789      4290\n",
      "weighted avg   0.944980  0.944522  0.944449      4290\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.575000  0.811765  0.673171       170\n",
      "         hap   0.810056  0.656109  0.725000       442\n",
      "         neu   0.624464  0.757812  0.684706       384\n",
      "         sad   0.813559  0.587755  0.682464       245\n",
      "\n",
      "    accuracy                       0.695407      1241\n",
      "   macro avg   0.705770  0.703360  0.691335      1241\n",
      "weighted avg   0.721121  0.695407  0.697035      1241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffd44f0",
   "metadata": {},
   "source": [
    "# xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4ffc066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def XGBresults(excluded_session):\n",
    "    \n",
    "    print(f\"Session{excluded_session} is used as test set.\")\n",
    "    x_train, y_train, x_test, y_test = load_data(excluded_session)\n",
    "    \n",
    "    bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic')\n",
    "    bst.fit(x_train, y_train)\n",
    "    \n",
    "    target_names = ['ang', 'hap', 'neu', 'sad']\n",
    "\n",
    "    train_predicted = bst.predict(x_train)\n",
    "    print('*** Train')\n",
    "    print(classification_report(y_train, train_predicted, target_names=target_names, digits=6))\n",
    "    print('*** Test')\n",
    "    test_predicted = bst.predict(x_test)\n",
    "    print(classification_report(y_test, test_predicted, target_names=target_names, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce83d578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session1 is used as test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:55:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.969838  0.956522  0.963134       874\n",
      "         hap   0.952381  0.957290  0.954829      1358\n",
      "         neu   0.925735  0.950906  0.938152      1324\n",
      "         sad   0.972061  0.938202  0.954831       890\n",
      "\n",
      "    accuracy                       0.951417      4446\n",
      "   macro avg   0.955004  0.950730  0.952737      4446\n",
      "weighted avg   0.951817  0.951417  0.951496      4446\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.887006  0.685590  0.773399       229\n",
      "         hap   0.666667  0.784173  0.720661       278\n",
      "         neu   0.654286  0.596354  0.623978       384\n",
      "         sad   0.614719  0.731959  0.668235       194\n",
      "\n",
      "    accuracy                       0.687558      1085\n",
      "   macro avg   0.705669  0.699519  0.696568      1085\n",
      "weighted avg   0.699501  0.687558  0.688200      1085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fee02897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session2 is used as test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:55:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.971338  0.947205  0.959119       966\n",
      "         hap   0.931060  0.959511  0.945071      1309\n",
      "         neu   0.929429  0.919762  0.924571      1346\n",
      "         sad   0.941243  0.939121  0.940181       887\n",
      "\n",
      "    accuracy                       0.940994      4508\n",
      "   macro avg   0.943267  0.941400  0.942236      4508\n",
      "weighted avg   0.941208  0.940994  0.940998      4508\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.816901  0.846715  0.831541       137\n",
      "         hap   0.712707  0.788991  0.748911       327\n",
      "         neu   0.731928  0.671271  0.700288       362\n",
      "         sad   0.796791  0.756345  0.776042       197\n",
      "\n",
      "    accuracy                       0.748778      1023\n",
      "   macro avg   0.764582  0.765831  0.764196      1023\n",
      "weighted avg   0.749654  0.748778  0.747996      1023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "557672c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session3 is used as test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:55:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.957647  0.943221  0.950379       863\n",
      "         hap   0.920059  0.920741  0.920400      1350\n",
      "         neu   0.907605  0.920029  0.913775      1388\n",
      "         sad   0.923575  0.915276  0.919407       779\n",
      "\n",
      "    accuracy                       0.923973      4380\n",
      "   macro avg   0.927222  0.924817  0.925990      4380\n",
      "weighted avg   0.924144  0.923973  0.924031      4380\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.722222  0.704167  0.713080       240\n",
      "         hap   0.738739  0.573427  0.645669       286\n",
      "         neu   0.584239  0.671875  0.625000       320\n",
      "         sad   0.663609  0.711475  0.686709       305\n",
      "\n",
      "    accuracy                       0.664639      1151\n",
      "   macro avg   0.677202  0.665236  0.667615      1151\n",
      "weighted avg   0.672432  0.664639  0.664854      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2163fd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session4 is used as test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:55:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.974734  0.944588  0.959424       776\n",
      "         hap   0.925461  0.940735  0.933036      1333\n",
      "         neu   0.920602  0.927586  0.924081      1450\n",
      "         sad   0.930258  0.921360  0.925788       941\n",
      "\n",
      "    accuracy                       0.933111      4500\n",
      "   macro avg   0.937764  0.933567  0.935582      4500\n",
      "weighted avg   0.933395  0.933111  0.933185      4500\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.763889  0.840979  0.800582       327\n",
      "         hap   0.677419  0.554455  0.609800       303\n",
      "         neu   0.584775  0.655039  0.617916       258\n",
      "         sad   0.664179  0.622378  0.642599       143\n",
      "\n",
      "    accuracy                       0.679922      1031\n",
      "   macro avg   0.672566  0.668213  0.667724      1031\n",
      "weighted avg   0.679825  0.679922  0.676890      1031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc4aebb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session5 is used as test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:55:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.978022  0.953912  0.965817       933\n",
      "         hap   0.961474  0.961474  0.961474      1194\n",
      "         neu   0.908364  0.943353  0.925528      1324\n",
      "         sad   0.943280  0.911800  0.927273       839\n",
      "\n",
      "    accuracy                       0.944522      4290\n",
      "   macro avg   0.947785  0.942635  0.945023      4290\n",
      "weighted avg   0.945123  0.944522  0.944636      4290\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.767296  0.717647  0.741641       170\n",
      "         hap   0.817439  0.678733  0.741656       442\n",
      "         neu   0.611219  0.822917  0.701443       384\n",
      "         sad   0.757576  0.612245  0.677201       245\n",
      "\n",
      "    accuracy                       0.715552      1241\n",
      "   macro avg   0.738382  0.707885  0.715485      1241\n",
      "weighted avg   0.734941  0.715552  0.716486      1241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4269f678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ba5e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
