{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a06ad505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84680165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(excluded_session):\n",
    "    train_text = torch.load(f\"./transcriptions/output_{excluded_session}/train_text_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    train_speech = torch.load(f\"./speech/output_{excluded_session}/train_speech_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    train_video = torch.load(f\"./video/output_{excluded_session}/train_video_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    concated_data = torch.cat((train_text, train_speech), 1)\n",
    "    concated_data = torch.cat((concated_data, train_video), 1)\n",
    "    \n",
    "    train_labels = torch.load(f\"./transcriptions/output_{excluded_session}/train_text_labels{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    \n",
    "    test_text = torch.load(f\"./transcriptions/output_{excluded_session}/test_text_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    test_speech = torch.load(f\"./speech/output_{excluded_session}/test_speech_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    test_video = torch.load(f\"./video/output_{excluded_session}/test_video_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    concated_test_data = torch.cat((test_text, test_speech), 1)\n",
    "    concated_test_data = torch.cat((concated_test_data, test_video), 1)\n",
    "\n",
    "    test_labels = torch.load(f\"./transcriptions/output_{excluded_session}/test_text_labels{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    \n",
    "    return concated_data, train_labels, concated_test_data, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c0d3ce",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1f7c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def SVM_results(excluded_session):\n",
    "    \n",
    "    print(f\"Session{excluded_session} is used as test set.\")\n",
    "    x_train, y_train, x_test, y_test = load_data(excluded_session)\n",
    "    \n",
    "    clf = svm.SVC()\n",
    "    clf.fit(x_train, y_train)\n",
    "    \n",
    "    target_names = ['ang', 'hap', 'neu', 'sad']\n",
    "\n",
    "    train_predicted = clf.predict(x_train)\n",
    "    print('*** Train')\n",
    "    print(classification_report(y_train, train_predicted, target_names=target_names, digits=6))\n",
    "    print('*** Test')\n",
    "    test_predicted = clf.predict(x_test)\n",
    "    print(classification_report(y_test, test_predicted, target_names=target_names, digits=6))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15ef1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session1 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.981818  0.988558  0.985177       874\n",
      "         hap   0.972263  0.980854  0.976540      1358\n",
      "         neu   0.967400  0.963746  0.965569      1324\n",
      "         sad   0.978335  0.964045  0.971138       890\n",
      "\n",
      "    accuracy                       0.973909      4446\n",
      "   macro avg   0.974954  0.974301  0.974606      4446\n",
      "weighted avg   0.973909  0.973909  0.973889      4446\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.892523  0.834061  0.862302       229\n",
      "         hap   0.700599  0.841727  0.764706       278\n",
      "         neu   0.792982  0.588542  0.675635       384\n",
      "         sad   0.630952  0.819588  0.713004       194\n",
      "\n",
      "    accuracy                       0.746544      1085\n",
      "   macro avg   0.754264  0.770979  0.753912      1085\n",
      "weighted avg   0.761350  0.746544  0.744537      1085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b74ff8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session2 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.973388  0.984472  0.978899       966\n",
      "         hap   0.972350  0.967150  0.969743      1309\n",
      "         neu   0.958395  0.958395  0.958395      1346\n",
      "         sad   0.966025  0.961669  0.963842       887\n",
      "\n",
      "    accuracy                       0.967169      4508\n",
      "   macro avg   0.967540  0.967922  0.967720      4508\n",
      "weighted avg   0.967161  0.967169  0.967156      4508\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.849673  0.948905  0.896552       137\n",
      "         hap   0.796970  0.804281  0.800609       327\n",
      "         neu   0.779412  0.732044  0.754986       362\n",
      "         sad   0.810000  0.822335  0.816121       197\n",
      "\n",
      "    accuracy                       0.801564      1023\n",
      "   macro avg   0.809014  0.826891  0.817067      1023\n",
      "weighted avg   0.800324  0.801564  0.800300      1023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = SVM_results(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "930723dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72bda7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c5b2f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session3 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.977984  0.977984  0.977984       863\n",
      "         hap   0.973763  0.962222  0.967958      1350\n",
      "         neu   0.948608  0.957493  0.953030      1388\n",
      "         sad   0.962916  0.966624  0.964766       779\n",
      "\n",
      "    accuracy                       0.964612      4380\n",
      "   macro avg   0.965818  0.966081  0.965934      4380\n",
      "weighted avg   0.964694  0.964612  0.964635      4380\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.743083  0.783333  0.762677       240\n",
      "         hap   0.811024  0.720280  0.762963       286\n",
      "         neu   0.635328  0.696875  0.664680       320\n",
      "         sad   0.778157  0.747541  0.762542       305\n",
      "\n",
      "    accuracy                       0.734144      1151\n",
      "   macro avg   0.741898  0.737007  0.738215      1151\n",
      "weighted avg   0.739301  0.734144  0.735467      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f701065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session4 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.980818  0.988402  0.984596       776\n",
      "         hap   0.974494  0.974494  0.974494      1333\n",
      "         neu   0.962707  0.961379  0.962043      1450\n",
      "         sad   0.970117  0.965994  0.968051       941\n",
      "\n",
      "    accuracy                       0.970889      4500\n",
      "   macro avg   0.972034  0.972567  0.972296      4500\n",
      "weighted avg   0.970871  0.970889  0.970877      4500\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.772251  0.902141  0.832158       327\n",
      "         hap   0.767273  0.696370  0.730104       303\n",
      "         neu   0.657588  0.655039  0.656311       258\n",
      "         sad   0.880342  0.720280  0.792308       143\n",
      "\n",
      "    accuracy                       0.754607      1031\n",
      "   macro avg   0.769363  0.743457  0.752720      1031\n",
      "weighted avg   0.757087  0.754607  0.752634      1031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f133480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session5 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.983906  0.982851  0.983378       933\n",
      "         hap   0.966997  0.981575  0.974231      1194\n",
      "         neu   0.956949  0.956949  0.956949      1324\n",
      "         sad   0.975669  0.955900  0.965683       839\n",
      "\n",
      "    accuracy                       0.969231      4290\n",
      "   macro avg   0.970880  0.969319  0.970060      4290\n",
      "weighted avg   0.969269  0.969231  0.969215      4290\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.703518  0.823529  0.758808       170\n",
      "         hap   0.810256  0.714932  0.759615       442\n",
      "         neu   0.653191  0.799479  0.718970       384\n",
      "         sad   0.824176  0.612245  0.702576       245\n",
      "\n",
      "    accuracy                       0.735697      1241\n",
      "   macro avg   0.747785  0.737546  0.734992      1241\n",
      "weighted avg   0.749782  0.735697  0.735667      1241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f25838c",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "745f259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_predict(model, d_loader):\n",
    "    predicted_set = []\n",
    "\n",
    "    # no need to calculate gradients during inference\n",
    "    with torch.no_grad():\n",
    "      for data in d_loader:\n",
    "        inputs, labels = data\n",
    "        # calculate output by running through the network\n",
    "        outputs = model(inputs)\n",
    "        # get the predictions\n",
    "        __, predicted = torch.max(outputs.data, 1)\n",
    "        predicted_set += predicted.tolist()\n",
    "    \n",
    "    return predicted_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de0c65bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_train(model, t_loader):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "    \n",
    "    epochs = 50\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(t_loader, 0):\n",
    "            inputs, labels = data\n",
    "            # set optimizer to zero grad to remove previous epoch gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward propagation\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # backward propagation\n",
    "            loss.backward()\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57916356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Data(Dataset):\n",
    "  def __init__(self, X_train, y_train):\n",
    "    # need to convert float64 to float32 else \n",
    "    # will get the following error\n",
    "    # RuntimeError: expected scalar type Double but found Float\n",
    "    #self.X = torch.from_numpy(X_train.cast(np.float64))\n",
    "    self.X = X_train\n",
    "    # need to convert float64 to Long else \n",
    "    # will get the following error\n",
    "    # RuntimeError: expected scalar type Long but found Float\n",
    "    self.y = torch.FloatTensor(y_train).type(torch.LongTensor)\n",
    "    self.len = self.X.shape[0]\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    return self.X[index], self.y[index]\n",
    "  def __len__(self):\n",
    "    return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cec3f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# number of features (len of X cols)\n",
    "input_dim = 2304\n",
    "# number of hidden layers\n",
    "hidden_layers1 = 768\n",
    "# number of classes (unique of y)\n",
    "hidden_layers2 = 256\n",
    "# number of classes (unique of y)\n",
    "output_dim = 4\n",
    "class Network(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Network, self).__init__()\n",
    "    self.linear1 = nn.Linear(input_dim, hidden_layers)\n",
    "    self.linear2 = nn.Linear(hidden_layers, output_dim)\n",
    "  def forward(self, x):\n",
    "    x = torch.sigmoid(self.linear1(x))\n",
    "    x = self.linear2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76fce30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def NN_results(excluded_session):\n",
    "    \n",
    "    print(f\"Session{excluded_session} is used as test set.\")\n",
    "    x_train, y_train, x_test, y_test = load_data(excluded_session)\n",
    "    \n",
    "    train_data = Data(x_train, y_train)\n",
    "    batch_size = 32\n",
    "    trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    clf = Network()\n",
    "    print(clf.parameters)\n",
    "    \n",
    "    NN_train(clf, trainloader)\n",
    "    \n",
    "    test_data = Data(x_test, y_test)\n",
    "    testloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    target_names = ['ang', 'hap', 'neu', 'sad']\n",
    "\n",
    "    train_predicted = NN_predict(clf, trainloader)\n",
    "    print('*** Train')\n",
    "    print(classification_report(y_train, train_predicted, target_names=target_names, digits=6))\n",
    "    print('*** Test')\n",
    "    test_predicted = NN_predict(clf, testloader)\n",
    "    print(classification_report(y_test, test_predicted, target_names=target_names, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b70a6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session1 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=2304, out_features=25, bias=True)\n",
      "  (linear2): Linear(in_features=25, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.994286  0.995423  0.994854       874\n",
      "         hap   0.991124  0.986745  0.988930      1358\n",
      "         neu   0.979806  0.989426  0.984592      1324\n",
      "         sad   0.992063  0.983146  0.987585       890\n",
      "\n",
      "    accuracy                       0.988529      4446\n",
      "   macro avg   0.989320  0.988685  0.988990      4446\n",
      "weighted avg   0.988563  0.988529  0.988533      4446\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.884615  0.803493  0.842105       229\n",
      "         hap   0.722772  0.787770  0.753873       278\n",
      "         neu   0.730303  0.627604  0.675070       384\n",
      "         sad   0.639344  0.804124  0.712329       194\n",
      "\n",
      "    accuracy                       0.737327      1085\n",
      "   macro avg   0.744259  0.755748  0.745844      1085\n",
      "weighted avg   0.744679  0.737327  0.737177      1085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86a19953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session2 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=2304, out_features=25, bias=True)\n",
      "  (linear2): Linear(in_features=25, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   1.000000  0.983437  0.991649       966\n",
      "         hap   0.985507  0.987013  0.986260      1309\n",
      "         neu   0.975373  0.971025  0.973194      1346\n",
      "         sad   0.968026  0.989853  0.978818       887\n",
      "\n",
      "    accuracy                       0.982032      4508\n",
      "   macro avg   0.982227  0.982832  0.982480      4508\n",
      "weighted avg   0.982147  0.982032  0.982049      4508\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.926230  0.824818  0.872587       137\n",
      "         hap   0.778711  0.850153  0.812865       327\n",
      "         neu   0.769912  0.720994  0.744650       362\n",
      "         sad   0.824390  0.857868  0.840796       197\n",
      "\n",
      "    accuracy                       0.802542      1023\n",
      "   macro avg   0.824811  0.813458  0.817725      1023\n",
      "weighted avg   0.804150  0.802542  0.802103      1023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f08ea83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session3 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=2304, out_features=25, bias=True)\n",
      "  (linear2): Linear(in_features=25, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.995365  0.995365  0.995365       863\n",
      "         hap   0.991031  0.982222  0.986607      1350\n",
      "         neu   0.974892  0.979107  0.976995      1388\n",
      "         sad   0.975796  0.983312  0.979540       779\n",
      "\n",
      "    accuracy                       0.984018      4380\n",
      "   macro avg   0.984271  0.985001  0.984627      4380\n",
      "weighted avg   0.984061  0.984018  0.984030      4380\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.776371  0.766667  0.771488       240\n",
      "         hap   0.810924  0.674825  0.736641       286\n",
      "         neu   0.640541  0.740625  0.686957       320\n",
      "         sad   0.781046  0.783607  0.782324       305\n",
      "\n",
      "    accuracy                       0.741095      1151\n",
      "   macro avg   0.752220  0.741431  0.744353      1151\n",
      "weighted avg   0.748432  0.741095  0.742199      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0a83eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session4 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=2304, out_features=25, bias=True)\n",
      "  (linear2): Linear(in_features=25, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.993548  0.992268  0.992908       776\n",
      "         hap   0.987906  0.980495  0.984187      1333\n",
      "         neu   0.976519  0.975172  0.975845      1450\n",
      "         sad   0.975891  0.989373  0.982586       941\n",
      "\n",
      "    accuracy                       0.982667      4500\n",
      "   macro avg   0.983466  0.984327  0.983881      4500\n",
      "weighted avg   0.982698  0.982667  0.982668      4500\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.759162  0.886850  0.818054       327\n",
      "         hap   0.788845  0.653465  0.714801       303\n",
      "         neu   0.649254  0.674419  0.661597       258\n",
      "         sad   0.853846  0.776224  0.813187       143\n",
      "\n",
      "    accuracy                       0.749758      1031\n",
      "   macro avg   0.762777  0.747739  0.751910      1031\n",
      "weighted avg   0.753515  0.749758  0.747882      1031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce84e315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session5 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=2304, out_features=25, bias=True)\n",
      "  (linear2): Linear(in_features=25, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.991453  0.994641  0.993044       933\n",
      "         hap   0.988235  0.984925  0.986577      1194\n",
      "         neu   0.973625  0.975831  0.974727      1324\n",
      "         sad   0.980884  0.978546  0.979714       839\n",
      "\n",
      "    accuracy                       0.982984      4290\n",
      "   macro avg   0.983549  0.983486  0.983515      4290\n",
      "weighted avg   0.982988  0.982984  0.982984      4290\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.702970  0.835294  0.763441       170\n",
      "         hap   0.834239  0.694570  0.758025       442\n",
      "         neu   0.638430  0.804688  0.711982       384\n",
      "         sad   0.823529  0.628571  0.712963       245\n",
      "\n",
      "    accuracy                       0.734891      1241\n",
      "   macro avg   0.749792  0.740781  0.736603      1241\n",
      "weighted avg   0.753554  0.734891  0.735623      1241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffd44f0",
   "metadata": {},
   "source": [
    "# xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4ffc066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def XGBresults(excluded_session):\n",
    "    \n",
    "    print(f\"Session{excluded_session} is used as test set.\")\n",
    "    x_train, y_train, x_test, y_test = load_data(excluded_session)\n",
    "    \n",
    "    bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic')\n",
    "    bst.fit(x_train, y_train)\n",
    "    \n",
    "    target_names = ['ang', 'hap', 'neu', 'sad']\n",
    "\n",
    "    train_predicted = bst.predict(x_train)\n",
    "    print('*** Train')\n",
    "    print(classification_report(y_train, train_predicted, target_names=target_names, digits=6))\n",
    "    print('*** Test')\n",
    "    test_predicted = bst.predict(x_test)\n",
    "    print(classification_report(y_test, test_predicted, target_names=target_names, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce83d578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session1 is used as test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:02:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.982798  0.980549  0.981672       874\n",
      "         hap   0.960841  0.975700  0.968213      1358\n",
      "         neu   0.958904  0.951662  0.955269      1324\n",
      "         sad   0.962543  0.952809  0.957651       890\n",
      "\n",
      "    accuracy                       0.964912      4446\n",
      "   macro avg   0.966272  0.965180  0.965701      4446\n",
      "weighted avg   0.964921  0.964912  0.964890      4446\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.914439  0.746725  0.822115       229\n",
      "         hap   0.667647  0.816547  0.734628       278\n",
      "         neu   0.744898  0.570312  0.646018       384\n",
      "         sad   0.587121  0.798969  0.676856       194\n",
      "\n",
      "    accuracy                       0.711521      1085\n",
      "   macro avg   0.728526  0.733138  0.719904      1085\n",
      "weighted avg   0.732677  0.711521  0.711403      1085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fee02897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session2 is used as test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:02:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.979317  0.980331  0.979824       966\n",
      "         hap   0.967300  0.971734  0.969512      1309\n",
      "         neu   0.959671  0.954681  0.957169      1346\n",
      "         sad   0.958286  0.958286  0.958286       887\n",
      "\n",
      "    accuracy                       0.965839      4508\n",
      "   macro avg   0.966144  0.966258  0.966198      4508\n",
      "weighted avg   0.965824  0.965839  0.965828      4508\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.846154  0.883212  0.864286       137\n",
      "         hap   0.760684  0.816514  0.787611       327\n",
      "         neu   0.767647  0.720994  0.743590       362\n",
      "         sad   0.820106  0.786802  0.803109       197\n",
      "\n",
      "    accuracy                       0.785924      1023\n",
      "   macro avg   0.798648  0.801880  0.799649      1023\n",
      "weighted avg   0.786037  0.785924  0.785286      1023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "557672c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session3 is used as test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:02:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.963387  0.975666  0.969488       863\n",
      "         hap   0.952522  0.951111  0.951816      1350\n",
      "         neu   0.934438  0.934438  0.934438      1388\n",
      "         sad   0.963636  0.952503  0.958037       779\n",
      "\n",
      "    accuracy                       0.950913      4380\n",
      "   macro avg   0.953496  0.953430  0.953445      4380\n",
      "weighted avg   0.950909  0.950913  0.950897      4380\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.732000  0.762500  0.746939       240\n",
      "         hap   0.785992  0.706294  0.744015       286\n",
      "         neu   0.608219  0.693750  0.648175       320\n",
      "         sad   0.767025  0.701639  0.732877       305\n",
      "\n",
      "    accuracy                       0.713293      1151\n",
      "   macro avg   0.723309  0.716046  0.718001      1151\n",
      "weighted avg   0.720284  0.713293  0.715028      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2163fd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session4 is used as test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:02:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.980695  0.981959  0.981326       776\n",
      "         hap   0.968373  0.964741  0.966554      1333\n",
      "         neu   0.955556  0.948966  0.952249      1450\n",
      "         sad   0.947644  0.961743  0.954641       941\n",
      "\n",
      "    accuracy                       0.962000      4500\n",
      "   macro avg   0.963067  0.964352  0.963693      4500\n",
      "weighted avg   0.962033  0.962000  0.962001      4500\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.727041  0.871560  0.792768       327\n",
      "         hap   0.754032  0.617162  0.678766       303\n",
      "         neu   0.635659  0.635659  0.635659       258\n",
      "         sad   0.812030  0.755245  0.782609       143\n",
      "\n",
      "    accuracy                       0.721629      1031\n",
      "   macro avg   0.732191  0.719906  0.722450      1031\n",
      "weighted avg   0.723894  0.721629  0.718539      1031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc4aebb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session5 is used as test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:14:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.986957  0.973205  0.980032       933\n",
      "         hap   0.962810  0.975712  0.969218      1194\n",
      "         neu   0.940120  0.948640  0.944361      1324\n",
      "         sad   0.963592  0.946365  0.954901       839\n",
      "\n",
      "    accuracy                       0.961072      4290\n",
      "   macro avg   0.963370  0.960980  0.962128      4290\n",
      "weighted avg   0.961212  0.961072  0.961098      4290\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.775148  0.770588  0.772861       170\n",
      "         hap   0.826816  0.669683  0.740000       442\n",
      "         neu   0.631579  0.843750  0.722408       384\n",
      "         sad   0.781095  0.640816  0.704036       245\n",
      "\n",
      "    accuracy                       0.731668      1241\n",
      "   macro avg   0.753659  0.731209  0.734826      1241\n",
      "weighted avg   0.750300  0.731668  0.731958      1241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4269f678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
