{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a06ad505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84680165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(excluded_session):\n",
    "    train_text = torch.load(f\"./transcriptions/output_{excluded_session}/train_text_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    train_speech = torch.load(f\"./speech/output_{excluded_session}/train_speech_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    train_video = torch.load(f\"./video/output_{excluded_session}/train_video_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    concated_data = torch.cat((train_text, train_speech), 1)\n",
    "    concated_data = torch.cat((concated_data, train_video), 1)\n",
    "    \n",
    "    train_labels = torch.load(f\"./transcriptions/output_{excluded_session}/train_text_labels{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    \n",
    "    test_text = torch.load(f\"./transcriptions/output_{excluded_session}/test_text_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    test_speech = torch.load(f\"./speech/output_{excluded_session}/test_speech_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    test_video = torch.load(f\"./video/output_{excluded_session}/test_video_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    concated_test_data = torch.cat((test_text, test_speech), 1)\n",
    "    concated_test_data = torch.cat((concated_test_data, test_video), 1)\n",
    "\n",
    "    test_labels = torch.load(f\"./transcriptions/output_{excluded_session}/test_text_labels{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    \n",
    "    return concated_data, train_labels, concated_test_data, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c0d3ce",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1f7c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def SVM_results(excluded_session):\n",
    "    \n",
    "    print(f\"Session{excluded_session} is used as test set.\")\n",
    "    x_train, y_train, x_test, y_test = load_data(excluded_session)\n",
    "    \n",
    "    clf = svm.SVC()\n",
    "    clf.fit(x_train, y_train)\n",
    "    \n",
    "    target_names = ['ang', 'hap', 'neu', 'sad']\n",
    "\n",
    "    train_predicted = clf.predict(x_train)\n",
    "    print('*** Train')\n",
    "    print(classification_report(y_train, train_predicted, target_names=target_names, digits=6))\n",
    "    print('*** Test')\n",
    "    test_predicted = clf.predict(x_test)\n",
    "    print(classification_report(y_test, test_predicted, target_names=target_names, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f15ef1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session1 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.959276  0.970252  0.964733       874\n",
      "         hap   0.926829  0.979381  0.952381      1358\n",
      "         neu   0.943716  0.924471  0.933995      1324\n",
      "         sad   0.975904  0.910112  0.941860       890\n",
      "\n",
      "    accuracy                       0.947368      4446\n",
      "   macro avg   0.951431  0.946054  0.948242      4446\n",
      "weighted avg   0.948060  0.947368  0.947228      4446\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.905473  0.794760  0.846512       229\n",
      "         hap   0.650273  0.856115  0.739130       278\n",
      "         neu   0.772881  0.593750  0.671576       384\n",
      "         sad   0.636771  0.731959  0.681055       194\n",
      "\n",
      "    accuracy                       0.728111      1085\n",
      "   macro avg   0.741350  0.744146  0.734568      1085\n",
      "weighted avg   0.745115  0.728111  0.727502      1085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b74ff8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session2 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.957732  0.961698  0.959711       966\n",
      "         hap   0.938852  0.961803  0.950189      1309\n",
      "         neu   0.947612  0.913819  0.930408      1346\n",
      "         sad   0.937709  0.950395  0.944009       887\n",
      "\n",
      "    accuracy                       0.945209      4508\n",
      "   macro avg   0.945476  0.946928  0.946079      4508\n",
      "weighted avg   0.945288  0.945209  0.945107      4508\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.843537  0.905109  0.873239       137\n",
      "         hap   0.763533  0.819572  0.790560       327\n",
      "         neu   0.783951  0.701657  0.740525       362\n",
      "         sad   0.800995  0.817259  0.809045       197\n",
      "\n",
      "    accuracy                       0.788856      1023\n",
      "   macro avg   0.798004  0.810899  0.803342      1023\n",
      "weighted avg   0.788686  0.788856  0.787487      1023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c5b2f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session3 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.951081  0.968714  0.959816       863\n",
      "         hap   0.949077  0.952593  0.950832      1350\n",
      "         neu   0.932130  0.930115  0.931122      1388\n",
      "         sad   0.959264  0.937099  0.948052       779\n",
      "\n",
      "    accuracy                       0.945890      4380\n",
      "   macro avg   0.947888  0.947130  0.947455      4380\n",
      "weighted avg   0.945913  0.945890  0.945862      4380\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.723077  0.783333  0.752000       240\n",
      "         hap   0.773234  0.727273  0.749550       286\n",
      "         neu   0.618911  0.675000  0.645740       320\n",
      "         sad   0.787546  0.704918  0.743945       305\n",
      "\n",
      "    accuracy                       0.718506      1151\n",
      "   macro avg   0.725692  0.722631  0.722809      1151\n",
      "weighted avg   0.723663  0.718506  0.719714      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f701065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session4 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.976501  0.963918  0.970169       776\n",
      "         hap   0.940190  0.966992  0.953402      1333\n",
      "         neu   0.938832  0.942069  0.940448      1450\n",
      "         sad   0.960352  0.926674  0.943213       941\n",
      "\n",
      "    accuracy                       0.950000      4500\n",
      "   macro avg   0.953969  0.949913  0.951808      4500\n",
      "weighted avg   0.950230  0.950000  0.949988      4500\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.795640  0.892966  0.841499       327\n",
      "         hap   0.757377  0.762376  0.759868       303\n",
      "         neu   0.637795  0.627907  0.632812       258\n",
      "         sad   0.847619  0.622378  0.717742       143\n",
      "\n",
      "    accuracy                       0.750727      1031\n",
      "   macro avg   0.759608  0.726407  0.737980      1031\n",
      "weighted avg   0.752105  0.750727  0.748121      1031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f133480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session5 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.957850  0.974277  0.965994       933\n",
      "         hap   0.936699  0.979062  0.957412      1194\n",
      "         neu   0.936826  0.918429  0.927536      1324\n",
      "         sad   0.962264  0.911800  0.936353       839\n",
      "\n",
      "    accuracy                       0.946154      4290\n",
      "   macro avg   0.948410  0.945892  0.946824      4290\n",
      "weighted avg   0.946338  0.946154  0.945939      4290\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.658537  0.794118  0.720000       170\n",
      "         hap   0.767059  0.737557  0.752018       442\n",
      "         neu   0.665860  0.716146  0.690088       384\n",
      "         sad   0.747475  0.604082  0.668172       245\n",
      "\n",
      "    accuracy                       0.712329      1241\n",
      "   macro avg   0.709732  0.712975  0.707569      1241\n",
      "weighted avg   0.717013  0.712329  0.711916      1241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f25838c",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "745f259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_predict(model, d_loader):\n",
    "    predicted_set = []\n",
    "\n",
    "    # no need to calculate gradients during inference\n",
    "    with torch.no_grad():\n",
    "      for data in d_loader:\n",
    "        inputs, labels = data\n",
    "        # calculate output by running through the network\n",
    "        outputs = model(inputs)\n",
    "        # get the predictions\n",
    "        __, predicted = torch.max(outputs.data, 1)\n",
    "        predicted_set += predicted.tolist()\n",
    "    \n",
    "    return predicted_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de0c65bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_train(model, t_loader):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "    \n",
    "    epochs = 50\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(t_loader, 0):\n",
    "            inputs, labels = data\n",
    "            # set optimizer to zero grad to remove previous epoch gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward propagation\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # backward propagation\n",
    "            loss.backward()\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57916356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Data(Dataset):\n",
    "  def __init__(self, X_train, y_train):\n",
    "    # need to convert float64 to float32 else \n",
    "    # will get the following error\n",
    "    # RuntimeError: expected scalar type Double but found Float\n",
    "    #self.X = torch.from_numpy(X_train.cast(np.float64))\n",
    "    self.X = X_train\n",
    "    # need to convert float64 to Long else \n",
    "    # will get the following error\n",
    "    # RuntimeError: expected scalar type Long but found Float\n",
    "    self.y = torch.FloatTensor(y_train).type(torch.LongTensor)\n",
    "    self.len = self.X.shape[0]\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    return self.X[index], self.y[index]\n",
    "  def __len__(self):\n",
    "    return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cec3f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# number of features (len of X cols)\n",
    "input_dim = 12\n",
    "# number of hidden layers\n",
    "hidden_layers = 25\n",
    "# number of classes (unique of y)\n",
    "output_dim = 4\n",
    "class Network(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Network, self).__init__()\n",
    "    self.linear1 = nn.Linear(input_dim, hidden_layers)\n",
    "    self.linear2 = nn.Linear(hidden_layers, output_dim)\n",
    "  def forward(self, x):\n",
    "    x = torch.sigmoid(self.linear1(x))\n",
    "    x = self.linear2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76fce30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def NN_results(excluded_session):\n",
    "    \n",
    "    print(f\"Session{excluded_session} is used as test set.\")\n",
    "    x_train, y_train, x_test, y_test = load_data(excluded_session)\n",
    "    \n",
    "    train_data = Data(x_train, y_train)\n",
    "    batch_size = 32\n",
    "    trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    clf = Network()\n",
    "    print(clf.parameters)\n",
    "    \n",
    "    NN_train(clf, trainloader)\n",
    "    \n",
    "    test_data = Data(x_test, y_test)\n",
    "    testloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    target_names = ['ang', 'hap', 'neu', 'sad']\n",
    "\n",
    "    train_predicted = NN_predict(clf, trainloader)\n",
    "    print('*** Train')\n",
    "    print(classification_report(y_train, train_predicted, target_names=target_names, digits=6))\n",
    "    print('*** Test')\n",
    "    test_predicted = NN_predict(clf, testloader)\n",
    "    print(classification_report(y_test, test_predicted, target_names=target_names, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b70a6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session1 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=12, out_features=25, bias=True)\n",
      "  (linear2): Linear(in_features=25, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.964932  0.975973  0.970421       874\n",
      "         hap   0.944006  0.968336  0.956016      1358\n",
      "         neu   0.943854  0.939577  0.941711      1324\n",
      "         sad   0.971798  0.929213  0.950029       890\n",
      "\n",
      "    accuracy                       0.953441      4446\n",
      "   macro avg   0.956148  0.953275  0.954544      4446\n",
      "weighted avg   0.953638  0.953441  0.953389      4446\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.880184  0.834061  0.856502       229\n",
      "         hap   0.676647  0.812950  0.738562       278\n",
      "         neu   0.757475  0.593750  0.665693       384\n",
      "         sad   0.643777  0.773196  0.702576       194\n",
      "\n",
      "    accuracy                       0.732719      1085\n",
      "   macro avg   0.739521  0.753489  0.740833      1085\n",
      "weighted avg   0.742335  0.732719  0.731231      1085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86a19953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session2 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=12, out_features=25, bias=True)\n",
      "  (linear2): Linear(in_features=25, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.966422  0.953416  0.959875       966\n",
      "         hap   0.941133  0.964859  0.952848      1309\n",
      "         neu   0.945802  0.920505  0.932982      1346\n",
      "         sad   0.933555  0.950395  0.941899       887\n",
      "\n",
      "    accuracy                       0.946318      4508\n",
      "   macro avg   0.946728  0.947294  0.946901      4508\n",
      "weighted avg   0.946455  0.946318  0.946268      4508\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.861314  0.861314  0.861314       137\n",
      "         hap   0.743316  0.850153  0.793153       327\n",
      "         neu   0.775244  0.657459  0.711510       362\n",
      "         sad   0.780488  0.812183  0.796020       197\n",
      "\n",
      "    accuracy                       0.776149      1023\n",
      "   macro avg   0.790090  0.795277  0.790499      1023\n",
      "weighted avg   0.777574  0.776149  0.773943      1023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f08ea83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session3 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=12, out_features=25, bias=True)\n",
      "  (linear2): Linear(in_features=25, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.958668  0.967555  0.963091       863\n",
      "         hap   0.942209  0.954074  0.948105      1350\n",
      "         neu   0.942308  0.917867  0.929927      1388\n",
      "         sad   0.939241  0.952503  0.945825       779\n",
      "\n",
      "    accuracy                       0.944977      4380\n",
      "   macro avg   0.945606  0.948000  0.946737      4380\n",
      "weighted avg   0.944955  0.944977  0.944892      4380\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.754032  0.779167  0.766393       240\n",
      "         hap   0.779026  0.727273  0.752260       286\n",
      "         neu   0.634503  0.678125  0.655589       320\n",
      "         sad   0.748299  0.721311  0.734558       305\n",
      "\n",
      "    accuracy                       0.722850      1151\n",
      "   macro avg   0.728965  0.726469  0.727200      1151\n",
      "weighted avg   0.725492  0.722850  0.723640      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0a83eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session4 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=12, out_features=25, bias=True)\n",
      "  (linear2): Linear(in_features=25, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.976744  0.974227  0.975484       776\n",
      "         hap   0.946284  0.964741  0.955423      1333\n",
      "         neu   0.955809  0.924828  0.940063      1450\n",
      "         sad   0.936722  0.959617  0.948031       941\n",
      "\n",
      "    accuracy                       0.952444      4500\n",
      "   macro avg   0.953890  0.955853  0.954750      4500\n",
      "weighted avg   0.952606  0.952444  0.952388      4500\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.801090  0.899083  0.847262       327\n",
      "         hap   0.764901  0.762376  0.763636       303\n",
      "         neu   0.668122  0.593023  0.628337       258\n",
      "         sad   0.827068  0.769231  0.797101       143\n",
      "\n",
      "    accuracy                       0.764306      1031\n",
      "   macro avg   0.765295  0.755928  0.759084      1031\n",
      "weighted avg   0.760783  0.764306  0.760944      1031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce84e315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session5 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=12, out_features=25, bias=True)\n",
      "  (linear2): Linear(in_features=25, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.962065  0.978564  0.970244       933\n",
      "         hap   0.945028  0.979062  0.961744      1194\n",
      "         neu   0.943323  0.917674  0.930322      1324\n",
      "         sad   0.955882  0.929678  0.942598       839\n",
      "\n",
      "    accuracy                       0.950350      4290\n",
      "   macro avg   0.951575  0.951244  0.951227      4290\n",
      "weighted avg   0.950330  0.950350  0.950151      4290\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.663462  0.811765  0.730159       170\n",
      "         hap   0.782082  0.730769  0.755556       442\n",
      "         neu   0.675545  0.726562  0.700125       384\n",
      "         sad   0.753623  0.636735  0.690265       245\n",
      "\n",
      "    accuracy                       0.721998      1241\n",
      "   macro avg   0.718678  0.726458  0.719026      1241\n",
      "weighted avg   0.727249  0.721998  0.722035      1241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffd44f0",
   "metadata": {},
   "source": [
    "# xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4ffc066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def XGBresults(excluded_session):\n",
    "    \n",
    "    print(f\"Session{excluded_session} is used as test set.\")\n",
    "    x_train, y_train, x_test, y_test = load_data(excluded_session)\n",
    "    \n",
    "    bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic')\n",
    "    bst.fit(x_train, y_train)\n",
    "    \n",
    "    target_names = ['ang', 'hap', 'neu', 'sad']\n",
    "\n",
    "    train_predicted = bst.predict(x_train)\n",
    "    print('*** Train')\n",
    "    print(classification_report(y_train, train_predicted, target_names=target_names, digits=6))\n",
    "    print('*** Test')\n",
    "    test_predicted = bst.predict(x_test)\n",
    "    print(classification_report(y_test, test_predicted, target_names=target_names, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce83d578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session1 is used as test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:09:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.972881  0.985126  0.978965       874\n",
      "         hap   0.960116  0.974963  0.967483      1358\n",
      "         neu   0.958557  0.943353  0.950895      1324\n",
      "         sad   0.963595  0.951685  0.957603       890\n",
      "\n",
      "    accuracy                       0.962888      4446\n",
      "   macro avg   0.963787  0.963782  0.963736      4446\n",
      "weighted avg   0.962858  0.962888  0.962822      4446\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.863636  0.829694  0.846325       229\n",
      "         hap   0.695385  0.812950  0.749585       278\n",
      "         neu   0.757377  0.601562  0.670537       384\n",
      "         sad   0.646809  0.783505  0.708625       194\n",
      "\n",
      "    accuracy                       0.736406      1085\n",
      "   macro avg   0.740802  0.756928  0.743768      1085\n",
      "weighted avg   0.744150  0.736406  0.734703      1085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fee02897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session2 is used as test set.\n",
      "[20:09:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.981289  0.977226  0.979253       966\n",
      "         hap   0.963581  0.970206  0.966882      1309\n",
      "         neu   0.949741  0.954681  0.952205      1346\n",
      "         sad   0.963429  0.950395  0.956867       887\n",
      "\n",
      "    accuracy                       0.963177      4508\n",
      "   macro avg   0.964510  0.963127  0.963802      4508\n",
      "weighted avg   0.963213  0.963177  0.963180      4508\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.860294  0.854015  0.857143       137\n",
      "         hap   0.756598  0.788991  0.772455       327\n",
      "         neu   0.749288  0.726519  0.737728       362\n",
      "         sad   0.805128  0.796954  0.801020       197\n",
      "\n",
      "    accuracy                       0.777126      1023\n",
      "   macro avg   0.792827  0.791620  0.792087      1023\n",
      "weighted avg   0.777244  0.777126  0.777009      1023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "XGBresults(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "557672c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session3 is used as test set.\n",
      "[20:09:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.963303  0.973349  0.968300       863\n",
      "         hap   0.952768  0.956296  0.954529      1350\n",
      "         neu   0.936736  0.938761  0.937747      1388\n",
      "         sad   0.954068  0.933248  0.943543       779\n",
      "\n",
      "    accuracy                       0.950000      4380\n",
      "   macro avg   0.951719  0.950413  0.951030      4380\n",
      "weighted avg   0.949994  0.950000  0.949970      4380\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.735632  0.800000  0.766467       240\n",
      "         hap   0.762264  0.706294  0.733212       286\n",
      "         neu   0.616848  0.709375  0.659884       320\n",
      "         sad   0.793774  0.668852  0.725979       305\n",
      "\n",
      "    accuracy                       0.716768      1151\n",
      "   macro avg   0.727130  0.721130  0.721385      1151\n",
      "weighted avg   0.724632  0.716768  0.717843      1151\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "XGBresults(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2163fd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session4 is used as test set.\n",
      "[20:09:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.977893  0.969072  0.973463       776\n",
      "         hap   0.954074  0.966242  0.960119      1333\n",
      "         neu   0.944751  0.943448  0.944099      1450\n",
      "         sad   0.962487  0.954304  0.958378       941\n",
      "\n",
      "    accuracy                       0.956889      4500\n",
      "   macro avg   0.959801  0.958266  0.959015      4500\n",
      "weighted avg   0.956937  0.956889  0.956894      4500\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.795918  0.834862  0.814925       327\n",
      "         hap   0.769759  0.739274  0.754209       303\n",
      "         neu   0.621212  0.635659  0.628352       258\n",
      "         sad   0.827068  0.769231  0.797101       143\n",
      "\n",
      "    accuracy                       0.747818      1031\n",
      "   macro avg   0.753489  0.744756  0.748647      1031\n",
      "weighted avg   0.748832  0.747818  0.747921      1031\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "XGBresults(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc4aebb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session5 is used as test set.\n",
      "[20:09:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.983766  0.974277  0.978998       933\n",
      "         hap   0.960363  0.974037  0.967152      1194\n",
      "         neu   0.941397  0.946375  0.943879      1324\n",
      "         sad   0.957524  0.940405  0.948888       839\n",
      "\n",
      "    accuracy                       0.958974      4290\n",
      "   macro avg   0.960763  0.958773  0.959729      4290\n",
      "weighted avg   0.959044  0.958974  0.958974      4290\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.748663  0.823529  0.784314       170\n",
      "         hap   0.824468  0.701357  0.757946       442\n",
      "         neu   0.652083  0.815104  0.724537       384\n",
      "         sad   0.813131  0.657143  0.726862       245\n",
      "\n",
      "    accuracy                       0.744561      1241\n",
      "   macro avg   0.759586  0.749283  0.748415      1241\n",
      "weighted avg   0.758505  0.744561  0.745084      1241\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "XGBresults(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4269f678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
