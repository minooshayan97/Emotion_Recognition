{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a06ad505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84680165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(excluded_session):\n",
    "    train_text = torch.load(f\"./transcriptions/output_{excluded_session}/train_text_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    train_speech = torch.load(f\"./speech/output_{excluded_session}/train_speech_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    train_video = torch.load(f\"./video/output_{excluded_session}/train_video_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    summed_data = train_text + train_speech + train_video\n",
    "    \n",
    "    train_labels = torch.load(f\"./transcriptions/output_{excluded_session}/train_text_labels{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    \n",
    "    test_text = torch.load(f\"./transcriptions/output_{excluded_session}/test_text_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    test_speech = torch.load(f\"./speech/output_{excluded_session}/test_speech_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    test_video = torch.load(f\"./video/output_{excluded_session}/test_video_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    summed_test_data = test_text + test_speech + test_video\n",
    "\n",
    "    test_labels = torch.load(f\"./transcriptions/output_{excluded_session}/test_text_labels{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    \n",
    "    return summed_data, train_labels, summed_test_data, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c0d3ce",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1f7c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def SVM_results(excluded_session):\n",
    "    \n",
    "    print(f\"Session{excluded_session} is used as test set.\")\n",
    "    x_train, y_train, x_test, y_test = load_data(excluded_session)\n",
    "    \n",
    "    clf = svm.SVC()\n",
    "    clf.fit(x_train, y_train)\n",
    "    \n",
    "    target_names = ['ang', 'hap', 'neu', 'sad']\n",
    "\n",
    "    train_predicted = clf.predict(x_train)\n",
    "    print('*** Train')\n",
    "    print(classification_report(y_train, train_predicted, target_names=target_names, digits=6))\n",
    "    print('*** Test')\n",
    "    test_predicted = clf.predict(x_test)\n",
    "    print(classification_report(y_test, test_predicted, target_names=target_names, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15ef1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session1 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.968000  0.969108  0.968553       874\n",
      "         hap   0.922491  0.981591  0.951124      1358\n",
      "         neu   0.946071  0.927492  0.936690      1324\n",
      "         sad   0.977053  0.908989  0.941793       890\n",
      "\n",
      "    accuracy                       0.948493      4446\n",
      "   macro avg   0.953404  0.946795  0.949540      4446\n",
      "weighted avg   0.949382  0.948493  0.948384      4446\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.870466  0.733624  0.796209       229\n",
      "         hap   0.616368  0.866906  0.720478       278\n",
      "         neu   0.776271  0.596354  0.674521       384\n",
      "         sad   0.674757  0.716495  0.695000       194\n",
      "\n",
      "    accuracy                       0.716129      1085\n",
      "   macro avg   0.734466  0.728345  0.721552      1085\n",
      "weighted avg   0.737031  0.716129  0.715641      1085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b74ff8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session2 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.976369  0.940994  0.958355       966\n",
      "         hap   0.919192  0.973262  0.945455      1309\n",
      "         neu   0.946759  0.911590  0.928842      1346\n",
      "         sad   0.937430  0.945885  0.941639       887\n",
      "\n",
      "    accuracy                       0.942547      4508\n",
      "   macro avg   0.944938  0.942933  0.943573      4508\n",
      "weighted avg   0.943264  0.942547  0.942508      4508\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.908333  0.795620  0.848249       137\n",
      "         hap   0.703883  0.886850  0.784844       327\n",
      "         neu   0.774194  0.662983  0.714286       362\n",
      "         sad   0.828729  0.761421  0.793651       197\n",
      "\n",
      "    accuracy                       0.771261      1023\n",
      "   macro avg   0.803785  0.776719  0.785257      1023\n",
      "weighted avg   0.780185  0.771261  0.770063      1023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c5b2f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session3 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.966746  0.943221  0.954839       863\n",
      "         hap   0.944934  0.953333  0.949115      1350\n",
      "         neu   0.927547  0.931556  0.929547      1388\n",
      "         sad   0.947570  0.951220  0.949391       779\n",
      "\n",
      "    accuracy                       0.944064      4380\n",
      "   macro avg   0.946699  0.944833  0.945723      4380\n",
      "weighted avg   0.944191  0.944064  0.944091      4380\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.799065  0.712500  0.753304       240\n",
      "         hap   0.773438  0.692308  0.730627       286\n",
      "         neu   0.606383  0.712500  0.655172       320\n",
      "         sad   0.734426  0.734426  0.734426       305\n",
      "\n",
      "    accuracy                       0.713293      1151\n",
      "   macro avg   0.728328  0.712933  0.718382      1151\n",
      "weighted avg   0.721999  0.713293  0.715384      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f701065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session4 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.945746  0.988402  0.966604       776\n",
      "         hap   0.951091  0.948237  0.949662      1333\n",
      "         neu   0.936141  0.950345  0.943190      1450\n",
      "         sad   0.975225  0.920298  0.946966       941\n",
      "\n",
      "    accuracy                       0.950000      4500\n",
      "   macro avg   0.952051  0.951820  0.951605      4500\n",
      "weighted avg   0.950399  0.950000  0.949934      4500\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.664368  0.883792  0.758530       327\n",
      "         hap   0.760000  0.501650  0.604374       303\n",
      "         neu   0.619718  0.682171  0.649446       258\n",
      "         sad   0.830357  0.650350  0.729412       143\n",
      "\n",
      "    accuracy                       0.688652      1031\n",
      "   macro avg   0.718611  0.679491  0.685441      1031\n",
      "weighted avg   0.704323  0.688652  0.681889      1031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f133480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session5 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.961864  0.973205  0.967501       933\n",
      "         hap   0.934557  0.980737  0.957090      1194\n",
      "         neu   0.945626  0.906344  0.925569      1324\n",
      "         sad   0.941748  0.924911  0.933253       839\n",
      "\n",
      "    accuracy                       0.945221      4290\n",
      "   macro avg   0.945949  0.946299  0.945853      4290\n",
      "weighted avg   0.945318  0.945221  0.944964      4290\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.658537  0.794118  0.720000       170\n",
      "         hap   0.765116  0.744344  0.754587       442\n",
      "         neu   0.673267  0.708333  0.690355       384\n",
      "         sad   0.752475  0.620408  0.680089       245\n",
      "\n",
      "    accuracy                       0.715552      1241\n",
      "   macro avg   0.712349  0.716801  0.711258      1241\n",
      "weighted avg   0.719600  0.715552  0.715267      1241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f25838c",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "745f259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_predict(model, d_loader):\n",
    "    predicted_set = []\n",
    "\n",
    "    # no need to calculate gradients during inference\n",
    "    with torch.no_grad():\n",
    "      for data in d_loader:\n",
    "        inputs, labels = data\n",
    "        # calculate output by running through the network\n",
    "        outputs = model(inputs)\n",
    "        # get the predictions\n",
    "        __, predicted = torch.max(outputs.data, 1)\n",
    "        predicted_set += predicted.tolist()\n",
    "    \n",
    "    return predicted_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de0c65bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_train(model, t_loader):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "    \n",
    "    epochs = 50\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(t_loader, 0):\n",
    "            inputs, labels = data\n",
    "            # set optimizer to zero grad to remove previous epoch gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward propagation\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # backward propagation\n",
    "            loss.backward()\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57916356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Data(Dataset):\n",
    "  def __init__(self, X_train, y_train):\n",
    "    # need to convert float64 to float32 else \n",
    "    # will get the following error\n",
    "    # RuntimeError: expected scalar type Double but found Float\n",
    "    #self.X = torch.from_numpy(X_train.cast(np.float64))\n",
    "    self.X = X_train\n",
    "    # need to convert float64 to Long else \n",
    "    # will get the following error\n",
    "    # RuntimeError: expected scalar type Long but found Float\n",
    "    self.y = torch.FloatTensor(y_train).type(torch.LongTensor)\n",
    "    self.len = self.X.shape[0]\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    return self.X[index], self.y[index]\n",
    "  def __len__(self):\n",
    "    return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cec3f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# number of features (len of X cols)\n",
    "input_dim = 4\n",
    "# number of hidden layers\n",
    "hidden_layers = 12\n",
    "# number of classes (unique of y)\n",
    "output_dim = 4\n",
    "class Network(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Network, self).__init__()\n",
    "    self.linear1 = nn.Linear(input_dim, hidden_layers)\n",
    "    self.linear2 = nn.Linear(hidden_layers, output_dim)\n",
    "  def forward(self, x):\n",
    "    x = torch.sigmoid(self.linear1(x))\n",
    "    x = self.linear2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76fce30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def NN_results(excluded_session):\n",
    "    \n",
    "    print(f\"Session{excluded_session} is used as test set.\")\n",
    "    x_train, y_train, x_test, y_test = load_data(excluded_session)\n",
    "    \n",
    "    train_data = Data(x_train, y_train)\n",
    "    batch_size = 32\n",
    "    trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    clf = Network()\n",
    "    print(clf.parameters)\n",
    "    \n",
    "    NN_train(clf, trainloader)\n",
    "    \n",
    "    test_data = Data(x_test, y_test)\n",
    "    testloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    target_names = ['ang', 'hap', 'neu', 'sad']\n",
    "\n",
    "    train_predicted = NN_predict(clf, trainloader)\n",
    "    print('*** Train')\n",
    "    print(classification_report(y_train, train_predicted, target_names=target_names, digits=6))\n",
    "    print('*** Test')\n",
    "    test_predicted = NN_predict(clf, testloader)\n",
    "    print(classification_report(y_test, test_predicted, target_names=target_names, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b70a6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session1 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=4, out_features=12, bias=True)\n",
      "  (linear2): Linear(in_features=12, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.971396  0.971396  0.971396       874\n",
      "         hap   0.938342  0.974963  0.956302      1358\n",
      "         neu   0.947248  0.935801  0.941489      1324\n",
      "         sad   0.969519  0.929213  0.948939       890\n",
      "\n",
      "    accuracy                       0.953441      4446\n",
      "   macro avg   0.956626  0.952843  0.954531      4446\n",
      "weighted avg   0.953733  0.953441  0.953384      4446\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.888889  0.768559  0.824356       229\n",
      "         hap   0.652893  0.852518  0.739470       278\n",
      "         neu   0.766892  0.591146  0.667647       384\n",
      "         sad   0.649123  0.762887  0.701422       194\n",
      "\n",
      "    accuracy                       0.726267      1085\n",
      "   macro avg   0.739449  0.743777  0.733224      1085\n",
      "weighted avg   0.742374  0.726267  0.725163      1085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86a19953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session2 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=4, out_features=12, bias=True)\n",
      "  (linear2): Linear(in_features=12, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.971307  0.946170  0.958574       966\n",
      "         hap   0.932103  0.964859  0.948198      1309\n",
      "         neu   0.949192  0.916048  0.932325      1346\n",
      "         sad   0.928806  0.956032  0.942222       887\n",
      "\n",
      "    accuracy                       0.944543      4508\n",
      "   macro avg   0.945352  0.945777  0.945330      4508\n",
      "weighted avg   0.944958  0.944543  0.944506      4508\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.877863  0.839416  0.858209       137\n",
      "         hap   0.721106  0.877676  0.791724       327\n",
      "         neu   0.785479  0.657459  0.715789       362\n",
      "         sad   0.811518  0.786802  0.798969       197\n",
      "\n",
      "    accuracy                       0.777126      1023\n",
      "   macro avg   0.798991  0.790338  0.791173      1023\n",
      "weighted avg   0.782288  0.777126  0.775153      1023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f08ea83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session3 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=4, out_features=12, bias=True)\n",
      "  (linear2): Linear(in_features=12, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.959350  0.957126  0.958237       863\n",
      "         hap   0.944200  0.952593  0.948378      1350\n",
      "         neu   0.935036  0.922911  0.928934      1388\n",
      "         sad   0.940280  0.949936  0.945083       779\n",
      "\n",
      "    accuracy                       0.943607      4380\n",
      "   macro avg   0.944716  0.945641  0.945158      4380\n",
      "weighted avg   0.943584  0.943607  0.943573      4380\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.790909  0.725000  0.756522       240\n",
      "         hap   0.775424  0.639860  0.701149       286\n",
      "         neu   0.618785  0.700000  0.656891       320\n",
      "         sad   0.687688  0.750820  0.717868       305\n",
      "\n",
      "    accuracy                       0.703736      1151\n",
      "   macro avg   0.718201  0.703920  0.708108      1151\n",
      "weighted avg   0.711855  0.703736  0.704821      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0a83eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session4 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=4, out_features=12, bias=True)\n",
      "  (linear2): Linear(in_features=12, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.953923  0.987113  0.970234       776\n",
      "         hap   0.948507  0.953488  0.950991      1333\n",
      "         neu   0.960289  0.917241  0.938272      1450\n",
      "         sad   0.927984  0.958555  0.943021       941\n",
      "\n",
      "    accuracy                       0.948667      4500\n",
      "   macro avg   0.947676  0.954099  0.950630      4500\n",
      "weighted avg   0.948946  0.948667  0.948545      4500\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.686461  0.883792  0.772727       327\n",
      "         hap   0.727273  0.554455  0.629213       303\n",
      "         neu   0.654472  0.624031  0.638889       258\n",
      "         sad   0.796992  0.741259  0.768116       143\n",
      "\n",
      "    accuracy                       0.702231      1031\n",
      "   macro avg   0.716299  0.700884  0.702236      1031\n",
      "weighted avg   0.705781  0.702231  0.696418      1031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce84e315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session5 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=4, out_features=12, bias=True)\n",
      "  (linear2): Linear(in_features=12, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.965994  0.974277  0.970117       933\n",
      "         hap   0.944220  0.978224  0.960921      1194\n",
      "         neu   0.943794  0.913142  0.928215      1324\n",
      "         sad   0.941035  0.932062  0.936527       839\n",
      "\n",
      "    accuracy                       0.948252      4290\n",
      "   macro avg   0.948761  0.949426  0.948945      4290\n",
      "weighted avg   0.948201  0.948252  0.948057      4290\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.678392  0.794118  0.731707       170\n",
      "         hap   0.780788  0.717195  0.747642       442\n",
      "         neu   0.661972  0.734375  0.696296       384\n",
      "         sad   0.752381  0.644898  0.694505       245\n",
      "\n",
      "    accuracy                       0.718775      1241\n",
      "   macro avg   0.718383  0.722646  0.717538      1241\n",
      "weighted avg   0.724388  0.718775  0.719081      1241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffd44f0",
   "metadata": {},
   "source": [
    "# xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4ffc066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def XGBresults(excluded_session):\n",
    "    \n",
    "    print(f\"Session{excluded_session} is used as test set.\")\n",
    "    x_train, y_train, x_test, y_test = load_data(excluded_session)\n",
    "    \n",
    "    bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic')\n",
    "    bst.fit(x_train, y_train)\n",
    "    \n",
    "    target_names = ['ang', 'hap', 'neu', 'sad']\n",
    "\n",
    "    train_predicted = bst.predict(x_train)\n",
    "    print('*** Train')\n",
    "    print(classification_report(y_train, train_predicted, target_names=target_names, digits=6))\n",
    "    print('*** Test')\n",
    "    test_predicted = bst.predict(x_test)\n",
    "    print(classification_report(y_test, test_predicted, target_names=target_names, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce83d578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session1 is used as test set.\n",
      "[20:13:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.967232  0.979405  0.973280       874\n",
      "         hap   0.947482  0.969809  0.958515      1358\n",
      "         neu   0.959343  0.926737  0.942758      1324\n",
      "         sad   0.950673  0.952809  0.951740       890\n",
      "\n",
      "    accuracy                       0.955466      4446\n",
      "   macro avg   0.956182  0.957190  0.956573      4446\n",
      "weighted avg   0.955535  0.955466  0.955369      4446\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.878505  0.820961  0.848758       229\n",
      "         hap   0.711599  0.816547  0.760469       278\n",
      "         neu   0.761905  0.583333  0.660767       384\n",
      "         sad   0.608527  0.809278  0.694690       194\n",
      "\n",
      "    accuracy                       0.733641      1085\n",
      "   macro avg   0.740134  0.757530  0.741171      1085\n",
      "weighted avg   0.746201  0.733641  0.732056      1085\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "XGBresults(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fee02897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session2 is used as test set.\n",
      "[20:13:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.966632  0.959627  0.963117       966\n",
      "         hap   0.946816  0.965623  0.956127      1309\n",
      "         neu   0.943662  0.945765  0.944712      1346\n",
      "         sad   0.960694  0.936866  0.948630       887\n",
      "\n",
      "    accuracy                       0.952751      4508\n",
      "   macro avg   0.954451  0.951970  0.953147      4508\n",
      "weighted avg   0.952851  0.952751  0.952742      4508\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.855072  0.861314  0.858182       137\n",
      "         hap   0.735065  0.865443  0.794944       327\n",
      "         neu   0.766871  0.690608  0.726744       362\n",
      "         sad   0.844828  0.746193  0.792453       197\n",
      "\n",
      "    accuracy                       0.780059      1023\n",
      "   macro avg   0.800459  0.790889  0.793081      1023\n",
      "weighted avg   0.783528  0.780059  0.778800      1023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "557672c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session3 is used as test set.\n",
      "[20:13:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.955479  0.969873  0.962622       863\n",
      "         hap   0.956166  0.953333  0.954748      1350\n",
      "         neu   0.940962  0.930115  0.935507      1388\n",
      "         sad   0.947837  0.956354  0.952077       779\n",
      "\n",
      "    accuracy                       0.949772      4380\n",
      "   macro avg   0.950111  0.952419  0.951238      4380\n",
      "weighted avg   0.949731  0.949772  0.949727      4380\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.750000  0.775000  0.762295       240\n",
      "         hap   0.793388  0.671329  0.727273       286\n",
      "         neu   0.609687  0.668750  0.637854       320\n",
      "         sad   0.716129  0.727869  0.721951       305\n",
      "\n",
      "    accuracy                       0.707211      1151\n",
      "   macro avg   0.717301  0.710737  0.712343      1151\n",
      "weighted avg   0.712796  0.707211  0.708305      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2163fd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session4 is used as test set.\n",
      "[20:13:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.971686  0.972938  0.972312       776\n",
      "         hap   0.959307  0.954989  0.957143      1333\n",
      "         neu   0.936913  0.962759  0.949660      1450\n",
      "         sad   0.970199  0.934113  0.951814       941\n",
      "\n",
      "    accuracy                       0.956222      4500\n",
      "   macro avg   0.959526  0.956200  0.957732      4500\n",
      "weighted avg   0.956503  0.956222  0.956233      4500\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.711340  0.844037  0.772028       327\n",
      "         hap   0.756881  0.544554  0.633397       303\n",
      "         neu   0.566667  0.724806  0.636054       258\n",
      "         sad   0.936842  0.622378  0.747899       143\n",
      "\n",
      "    accuracy                       0.695441      1031\n",
      "   macro avg   0.742932  0.683944  0.697345      1031\n",
      "weighted avg   0.719798  0.695441  0.693913      1031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc4aebb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session5 is used as test set.\n",
      "[20:13:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.974277  0.974277  0.974277       933\n",
      "         hap   0.962469  0.966499  0.964480      1194\n",
      "         neu   0.922116  0.947885  0.934823      1324\n",
      "         sad   0.966123  0.917759  0.941320       839\n",
      "\n",
      "    accuracy                       0.952914      4290\n",
      "   macro avg   0.956246  0.951605  0.953725      4290\n",
      "weighted avg   0.953298  0.952914  0.952928      4290\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.697917  0.788235  0.740331       170\n",
      "         hap   0.810026  0.694570  0.747868       442\n",
      "         neu   0.637527  0.778646  0.701055       384\n",
      "         sad   0.771144  0.632653  0.695067       245\n",
      "\n",
      "    accuracy                       0.721193      1241\n",
      "   macro avg   0.729153  0.723526  0.721081      1241\n",
      "weighted avg   0.733617  0.721193  0.721927      1241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4269f678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
