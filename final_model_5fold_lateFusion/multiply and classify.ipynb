{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a06ad505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84680165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(excluded_session):\n",
    "    train_text = torch.load(f\"./transcriptions/output_{excluded_session}/train_text_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    train_speech = torch.load(f\"./speech/output_{excluded_session}/train_speech_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    train_video = torch.load(f\"./video/output_{excluded_session}/train_video_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    summed_data = train_text * train_speech * train_video\n",
    "    \n",
    "    train_labels = torch.load(f\"./transcriptions/output_{excluded_session}/train_text_labels{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    \n",
    "    test_text = torch.load(f\"./transcriptions/output_{excluded_session}/test_text_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    test_speech = torch.load(f\"./speech/output_{excluded_session}/test_speech_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    test_video = torch.load(f\"./video/output_{excluded_session}/test_video_{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    summed_test_data = test_text * test_speech * test_video\n",
    "\n",
    "    test_labels = torch.load(f\"./transcriptions/output_{excluded_session}/test_text_labels{excluded_session}.pt\",map_location=torch.device('cpu'))\n",
    "    \n",
    "    return summed_data, train_labels, summed_test_data, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c0d3ce",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1f7c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def SVM_results(excluded_session):\n",
    "    \n",
    "    print(f\"Session{excluded_session} is used as test set.\")\n",
    "    x_train, y_train, x_test, y_test = load_data(excluded_session)\n",
    "    \n",
    "    clf = svm.SVC()\n",
    "    clf.fit(x_train, y_train)\n",
    "    \n",
    "    target_names = ['ang', 'hap', 'neu', 'sad']\n",
    "\n",
    "    train_predicted = clf.predict(x_train)\n",
    "    print('*** Train')\n",
    "    print(classification_report(y_train, train_predicted, target_names=target_names, digits=6))\n",
    "    print('*** Test')\n",
    "    test_predicted = clf.predict(x_test)\n",
    "    print(classification_report(y_test, test_predicted, target_names=target_names, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f15ef1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session1 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.990160  0.921053  0.954357       874\n",
      "         hap   0.972635  0.916053  0.943496      1358\n",
      "         neu   0.842623  0.970544  0.902071      1324\n",
      "         sad   0.980700  0.913483  0.945899       890\n",
      "\n",
      "    accuracy                       0.932749      4446\n",
      "   macro avg   0.946529  0.930283  0.936456      4446\n",
      "weighted avg   0.938977  0.932749  0.933776      4446\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.966216  0.624454  0.758621       229\n",
      "         hap   0.762295  0.669065  0.712644       278\n",
      "         neu   0.573737  0.739583  0.646189       384\n",
      "         sad   0.691919  0.706186  0.698980       194\n",
      "\n",
      "    accuracy                       0.691244      1085\n",
      "   macro avg   0.748542  0.684822  0.704108      1085\n",
      "weighted avg   0.726018  0.691244  0.696385      1085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b74ff8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session2 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.997696  0.896480  0.944384       966\n",
      "         hap   0.812736  0.984721  0.890501      1309\n",
      "         neu   0.965323  0.889302  0.925754      1346\n",
      "         sad   0.976658  0.896280  0.934744       887\n",
      "\n",
      "    accuracy                       0.919920      4508\n",
      "   macro avg   0.938103  0.916696  0.923846      4508\n",
      "weighted avg   0.930183  0.919920  0.921279      4508\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.980583  0.737226  0.841667       137\n",
      "         hap   0.594862  0.920489  0.722689       327\n",
      "         neu   0.831461  0.613260  0.705882       362\n",
      "         sad   0.904762  0.675127  0.773256       197\n",
      "\n",
      "    accuracy                       0.739980      1023\n",
      "   macro avg   0.827917  0.736526  0.760873      1023\n",
      "weighted avg   0.789918  0.739980  0.742413      1023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c5b2f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session3 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.986216  0.911935  0.947622       863\n",
      "         hap   0.885517  0.951111  0.917143      1350\n",
      "         neu   0.919435  0.937320  0.928291      1388\n",
      "         sad   0.974895  0.897304  0.934492       779\n",
      "\n",
      "    accuracy                       0.929452      4380\n",
      "   macro avg   0.941516  0.924418  0.931887      4380\n",
      "weighted avg   0.932003  0.929452  0.929767      4380\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.888889  0.633333  0.739659       240\n",
      "         hap   0.549223  0.741259  0.630952       286\n",
      "         neu   0.598404  0.703125  0.646552       320\n",
      "         sad   0.862385  0.616393  0.718929       305\n",
      "\n",
      "    accuracy                       0.675065      1151\n",
      "   macro avg   0.724725  0.673528  0.684023      1151\n",
      "weighted avg   0.716705  0.675065  0.681269      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f701065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session4 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.995620  0.878866  0.933607       776\n",
      "         hap   0.982041  0.902476  0.940579      1333\n",
      "         neu   0.819284  0.978621  0.891892      1450\n",
      "         sad   0.980186  0.893730  0.934964       941\n",
      "\n",
      "    accuracy                       0.921111      4500\n",
      "   macro avg   0.944283  0.913423  0.925260      4500\n",
      "weighted avg   0.931551  0.921111  0.922514      4500\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.950893  0.651376  0.773140       327\n",
      "         hap   0.841060  0.419142  0.559471       303\n",
      "         neu   0.401770  0.879845  0.551640       258\n",
      "         sad   0.956044  0.608392  0.743590       143\n",
      "\n",
      "    accuracy                       0.634336      1031\n",
      "   macro avg   0.787442  0.639689  0.656960      1031\n",
      "weighted avg   0.781915  0.634336  0.650818      1031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f133480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session5 is used as test set.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.994145  0.909968  0.950196       933\n",
      "         hap   0.829441  0.981575  0.899118      1194\n",
      "         neu   0.945184  0.911631  0.928105      1324\n",
      "         sad   0.986595  0.877235  0.928707       839\n",
      "\n",
      "    accuracy                       0.924009      4290\n",
      "   macro avg   0.938841  0.920102  0.926531      4290\n",
      "weighted avg   0.931717  0.924009  0.924959      4290\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.895652  0.605882  0.722807       170\n",
      "         hap   0.618629  0.796380  0.696340       442\n",
      "         neu   0.669065  0.726562  0.696629       384\n",
      "         sad   0.878571  0.502041  0.638961       245\n",
      "\n",
      "    accuracy                       0.690572      1241\n",
      "   macro avg   0.765479  0.657716  0.688684      1241\n",
      "weighted avg   0.723502  0.690572  0.688727      1241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM_results(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f25838c",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "745f259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_predict(model, d_loader):\n",
    "    predicted_set = []\n",
    "\n",
    "    # no need to calculate gradients during inference\n",
    "    with torch.no_grad():\n",
    "      for data in d_loader:\n",
    "        inputs, labels = data\n",
    "        # calculate output by running through the network\n",
    "        outputs = model(inputs)\n",
    "        # get the predictions\n",
    "        __, predicted = torch.max(outputs.data, 1)\n",
    "        predicted_set += predicted.tolist()\n",
    "    \n",
    "    return predicted_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de0c65bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_train(model, t_loader):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "    \n",
    "    epochs = 50\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(t_loader, 0):\n",
    "            inputs, labels = data\n",
    "            # set optimizer to zero grad to remove previous epoch gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward propagation\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # backward propagation\n",
    "            loss.backward()\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57916356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Data(Dataset):\n",
    "  def __init__(self, X_train, y_train):\n",
    "    # need to convert float64 to float32 else \n",
    "    # will get the following error\n",
    "    # RuntimeError: expected scalar type Double but found Float\n",
    "    #self.X = torch.from_numpy(X_train.cast(np.float64))\n",
    "    self.X = X_train\n",
    "    # need to convert float64 to Long else \n",
    "    # will get the following error\n",
    "    # RuntimeError: expected scalar type Long but found Float\n",
    "    self.y = torch.FloatTensor(y_train).type(torch.LongTensor)\n",
    "    self.len = self.X.shape[0]\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    return self.X[index], self.y[index]\n",
    "  def __len__(self):\n",
    "    return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cec3f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# number of features (len of X cols)\n",
    "input_dim = 4\n",
    "# number of hidden layers\n",
    "hidden_layers = 12\n",
    "# number of classes (unique of y)\n",
    "output_dim = 4\n",
    "class Network(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Network, self).__init__()\n",
    "    self.linear1 = nn.Linear(input_dim, hidden_layers)\n",
    "    self.linear2 = nn.Linear(hidden_layers, output_dim)\n",
    "  def forward(self, x):\n",
    "    x = torch.sigmoid(self.linear1(x))\n",
    "    x = self.linear2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76fce30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def NN_results(excluded_session):\n",
    "    \n",
    "    print(f\"Session{excluded_session} is used as test set.\")\n",
    "    x_train, y_train, x_test, y_test = load_data(excluded_session)\n",
    "    \n",
    "    train_data = Data(x_train, y_train)\n",
    "    batch_size = 32\n",
    "    trainloader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    clf = Network()\n",
    "    print(clf.parameters)\n",
    "    \n",
    "    NN_train(clf, trainloader)\n",
    "    \n",
    "    test_data = Data(x_test, y_test)\n",
    "    testloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    target_names = ['ang', 'hap', 'neu', 'sad']\n",
    "\n",
    "    train_predicted = NN_predict(clf, trainloader)\n",
    "    print('*** Train')\n",
    "    print(classification_report(y_train, train_predicted, target_names=target_names, digits=6))\n",
    "    print('*** Test')\n",
    "    test_predicted = NN_predict(clf, testloader)\n",
    "    print(classification_report(y_test, test_predicted, target_names=target_names, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b70a6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session1 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=4, out_features=12, bias=True)\n",
      "  (linear2): Linear(in_features=12, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.711601  0.996568  0.830315       874\n",
      "         hap   0.980551  0.891016  0.933642      1358\n",
      "         neu   0.981356  0.874622  0.924920      1324\n",
      "         sad   0.985149  0.894382  0.937574       890\n",
      "\n",
      "    accuracy                       0.907557      4446\n",
      "   macro avg   0.914664  0.914147  0.906613      4446\n",
      "weighted avg   0.928841  0.907557  0.911519      4446\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.442589  0.925764  0.598870       229\n",
      "         hap   0.785047  0.604317  0.682927       278\n",
      "         neu   0.827586  0.437500  0.572402       384\n",
      "         sad   0.698413  0.680412  0.689295       194\n",
      "\n",
      "    accuracy                       0.626728      1085\n",
      "   macro avg   0.688409  0.661998  0.635873      1085\n",
      "weighted avg   0.712333  0.626728  0.627208      1085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86a19953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session2 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=4, out_features=12, bias=True)\n",
      "  (linear2): Linear(in_features=12, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.995465  0.908903  0.950216       966\n",
      "         hap   0.818933  0.984721  0.894207      1309\n",
      "         neu   0.963052  0.890788  0.925511      1346\n",
      "         sad   0.977695  0.889515  0.931523       887\n",
      "\n",
      "    accuracy                       0.921695      4508\n",
      "   macro avg   0.938786  0.918482  0.925365      4508\n",
      "weighted avg   0.931031  0.921695  0.922898      4508\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.971698  0.751825  0.847737       137\n",
      "         hap   0.597222  0.920489  0.724428       327\n",
      "         neu   0.835821  0.618785  0.711111       362\n",
      "         sad   0.903448  0.664975  0.766082       197\n",
      "\n",
      "    accuracy                       0.741935      1023\n",
      "   macro avg   0.827047  0.739018  0.762340      1023\n",
      "weighted avg   0.790773  0.741935  0.744251      1023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f08ea83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session3 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=4, out_features=12, bias=True)\n",
      "  (linear2): Linear(in_features=12, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.840551  0.989571  0.908994       863\n",
      "         hap   0.955157  0.946667  0.950893      1350\n",
      "         neu   0.946415  0.903458  0.924438      1388\n",
      "         sad   0.982882  0.884467  0.931081       779\n",
      "\n",
      "    accuracy                       0.930365      4380\n",
      "   macro avg   0.931251  0.931041  0.928851      4380\n",
      "weighted avg   0.934737  0.930365  0.930730      4380\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.515815  0.883333  0.651306       240\n",
      "         hap   0.787234  0.646853  0.710173       286\n",
      "         neu   0.637584  0.593750  0.614887       320\n",
      "         sad   0.869565  0.590164  0.703125       305\n",
      "\n",
      "    accuracy                       0.666377      1151\n",
      "   macro avg   0.702550  0.678525  0.669873      1151\n",
      "weighted avg   0.710850  0.666377  0.669539      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0a83eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session4 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=4, out_features=12, bias=True)\n",
      "  (linear2): Linear(in_features=12, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.675131  0.993557  0.803962       776\n",
      "         hap   0.981952  0.897974  0.938088      1333\n",
      "         neu   0.972393  0.874483  0.920842      1450\n",
      "         sad   0.982036  0.871413  0.923423       941\n",
      "\n",
      "    accuracy                       0.901333      4500\n",
      "   macro avg   0.902878  0.909357  0.896579      4500\n",
      "weighted avg   0.925980  0.901333  0.906335      4500\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.500000  0.944954  0.653968       327\n",
      "         hap   0.840000  0.415842  0.556291       303\n",
      "         neu   0.724719  0.500000  0.591743       258\n",
      "         sad   0.964706  0.573427  0.719298       143\n",
      "\n",
      "    accuracy                       0.626576      1031\n",
      "   macro avg   0.757356  0.608556  0.630325      1031\n",
      "weighted avg   0.720612  0.626576  0.618752      1031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce84e315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session5 is used as test set.\n",
      "<bound method Module.parameters of Network(\n",
      "  (linear1): Linear(in_features=4, out_features=12, bias=True)\n",
      "  (linear2): Linear(in_features=12, out_features=4, bias=True)\n",
      ")>\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.993080  0.922830  0.956667       933\n",
      "         hap   0.835949  0.981575  0.902928      1194\n",
      "         neu   0.944444  0.911631  0.927748      1324\n",
      "         sad   0.986541  0.873659  0.926675       839\n",
      "\n",
      "    accuracy                       0.926107      4290\n",
      "   macro avg   0.940003  0.922424  0.928504      4290\n",
      "weighted avg   0.933058  0.926107  0.926919      4290\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.885246  0.635294  0.739726       170\n",
      "         hap   0.621239  0.794118  0.697120       442\n",
      "         neu   0.665865  0.721354  0.692500       384\n",
      "         sad   0.876812  0.493878  0.631854       245\n",
      "\n",
      "    accuracy                       0.690572      1241\n",
      "   macro avg   0.762290  0.661161  0.690300      1241\n",
      "weighted avg   0.721668  0.690572  0.688642      1241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NN_results(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffd44f0",
   "metadata": {},
   "source": [
    "# xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4ffc066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def XGBresults(excluded_session):\n",
    "    \n",
    "    print(f\"Session{excluded_session} is used as test set.\")\n",
    "    x_train, y_train, x_test, y_test = load_data(excluded_session)\n",
    "    \n",
    "    bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic')\n",
    "    bst.fit(x_train, y_train)\n",
    "    \n",
    "    target_names = ['ang', 'hap', 'neu', 'sad']\n",
    "\n",
    "    train_predicted = bst.predict(x_train)\n",
    "    print('*** Train')\n",
    "    print(classification_report(y_train, train_predicted, target_names=target_names, digits=6))\n",
    "    print('*** Test')\n",
    "    test_predicted = bst.predict(x_test)\n",
    "    print(classification_report(y_test, test_predicted, target_names=target_names, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce83d578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session1 is used as test set.\n",
      "[13:16:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.978186  0.974828  0.976504       874\n",
      "         hap   0.967189  0.955081  0.961097      1358\n",
      "         neu   0.930556  0.961480  0.945765      1324\n",
      "         sad   0.972286  0.946067  0.958998       890\n",
      "\n",
      "    accuracy                       0.959064      4446\n",
      "   macro avg   0.962054  0.959364  0.960591      4446\n",
      "weighted avg   0.959462  0.959064  0.959140      4446\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.927184  0.834061  0.878161       229\n",
      "         hap   0.715655  0.805755  0.758037       278\n",
      "         neu   0.747692  0.632812  0.685472       384\n",
      "         sad   0.630705  0.783505  0.698851       194\n",
      "\n",
      "    accuracy                       0.746544      1085\n",
      "   macro avg   0.755309  0.764034  0.755130      1085\n",
      "weighted avg   0.756450  0.746544  0.747126      1085\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "XGBresults(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fee02897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session2 is used as test set.\n",
      "[13:16:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.970954  0.968944  0.969948       966\n",
      "         hap   0.962006  0.967150  0.964571      1309\n",
      "         neu   0.946746  0.950966  0.948851      1346\n",
      "         sad   0.957763  0.945885  0.951787       887\n",
      "\n",
      "    accuracy                       0.958518      4508\n",
      "   macro avg   0.959367  0.958236  0.958789      4508\n",
      "weighted avg   0.958532  0.958518  0.958514      4508\n",
      "\n",
      "*** Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.883212  0.883212  0.883212       137\n",
      "         hap   0.772727  0.831804  0.801178       327\n",
      "         neu   0.750000  0.712707  0.730878       362\n",
      "         sad   0.821053  0.791878  0.806202       197\n",
      "\n",
      "    accuracy                       0.788856      1023\n",
      "   macro avg   0.806748  0.804900  0.805367      1023\n",
      "weighted avg   0.788787  0.788856  0.788255      1023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "557672c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session3 is used as test set.\n",
      "[13:16:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.961187  0.975666  0.968373       863\n",
      "         hap   0.957778  0.957778  0.957778      1350\n",
      "         neu   0.941860  0.933718  0.937771      1388\n",
      "         sad   0.948586  0.947368  0.947977       779\n",
      "\n",
      "    accuracy                       0.951826      4380\n",
      "   macro avg   0.952353  0.953633  0.952975      4380\n",
      "weighted avg   0.951771  0.951826  0.951782      4380\n",
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.762295  0.775000  0.768595       240\n",
      "         hap   0.781377  0.674825  0.724203       286\n",
      "         neu   0.614958  0.693750  0.651982       320\n",
      "         sad   0.729097  0.714754  0.721854       305\n",
      "\n",
      "    accuracy                       0.711555      1151\n",
      "   macro avg   0.721932  0.714582  0.716659      1151\n",
      "weighted avg   0.717277  0.711555  0.712758      1151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2163fd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session4 is used as test set.\n",
      "[13:16:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.974392  0.980670  0.977521       776\n",
      "         hap   0.966063  0.960990  0.963520      1333\n",
      "         neu   0.951724  0.951724  0.951724      1450\n",
      "         sad   0.958643  0.960680  0.959660       941\n",
      "\n",
      "    accuracy                       0.961333      4500\n",
      "   macro avg   0.962705  0.963516  0.963106      4500\n",
      "weighted avg   0.961327  0.961333  0.961326      4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.805158  0.859327  0.831361       327\n",
      "         hap   0.794224  0.726073  0.758621       303\n",
      "         neu   0.629091  0.670543  0.649156       258\n",
      "         sad   0.853846  0.776224  0.813187       143\n",
      "\n",
      "    accuracy                       0.761397      1031\n",
      "   macro avg   0.770580  0.758042  0.763081      1031\n",
      "weighted avg   0.764638  0.761397  0.761867      1031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc4aebb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session5 is used as test set.\n",
      "[13:16:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "*** Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.983749  0.973205  0.978448       933\n",
      "         hap   0.951639  0.972362  0.961889      1194\n",
      "         neu   0.938623  0.947130  0.942857      1324\n",
      "         sad   0.971640  0.939213  0.955152       839\n",
      "\n",
      "    accuracy                       0.958275      4290\n",
      "   macro avg   0.961413  0.957977  0.959586      4290\n",
      "weighted avg   0.958517  0.958275  0.958299      4290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mini\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ang   0.747253  0.800000  0.772727       170\n",
      "         hap   0.832898  0.721719  0.773333       442\n",
      "         neu   0.647799  0.804688  0.717770       384\n",
      "         sad   0.819095  0.665306  0.734234       245\n",
      "\n",
      "    accuracy                       0.746978      1241\n",
      "   macro avg   0.761761  0.747928  0.749516      1241\n",
      "weighted avg   0.761166  0.746978  0.748338      1241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBresults(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4269f678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
